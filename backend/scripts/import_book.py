"""
EPUB ä¹¦ç±å¯¼å…¥è„šæœ¬
ç”¨æ³•: python import_book.py <epub_file> [--level <level>]
"""
import argparse
import os
import re
import sys
import uuid
import shutil
import logging
import zipfile
from collections import Counter
from typing import Dict, Optional, Set, Tuple

import ebooklib
from ebooklib import epub
from bs4 import BeautifulSoup
from xml.etree import ElementTree as ET

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from app.models.database import SessionLocal, create_tables, Book, Chapter, BookVocabulary
from app.utils.oss_helper import oss_helper
from app.utils.supabase_client import supabase_client

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def extract_text_from_html(html_content: str) -> str:
    """ä» HTML ä¸­æå–çº¯æ–‡æœ¬"""
    soup = BeautifulSoup(html_content, 'html.parser')
    return soup.get_text(separator=' ', strip=True)


def extract_words(text: str) -> list:
    """æå–æ–‡æœ¬ä¸­çš„å•è¯"""
    # åªä¿ç•™è‹±æ–‡å•è¯ï¼Œè½¬å°å†™
    words = re.findall(r'\b[a-zA-Z]+\b', text.lower())
    # è¿‡æ»¤æ‰å¤ªçŸ­çš„è¯
    return [w for w in words if len(w) > 2]


def detect_chapter_type(soup, text: str) -> tuple[str, str]:
    """
    æ£€æµ‹ç« èŠ‚ç±»å‹å¹¶æå–æ ‡é¢˜
    è¿”å›: (chapter_type, title)
    chapter_type: 'cover', 'toc', 'frontmatter', 'chapter', 'testimonial', 'skip'
    """
    # æ ‡å‡†åŒ–å¼•å·å’Œå…¶ä»–ç‰¹æ®Šå­—ç¬¦
    text_normalized = text.replace(''', "'").replace(''', "'").replace('"', '"').replace('"', '"')
    text_lower = text_normalized.lower()
    images = soup.find_all('img')

    # å…ˆè·å–æ ‡é¢˜æ ‡ç­¾å†…å®¹
    title_tag = soup.find(['h1', 'h2', 'h3'])
    title_text = title_tag.get_text(strip=True) if title_tag else ''
    title_lower = title_text.lower()

    # æ£€æµ‹å°é¢ï¼šæœ‰å›¾ç‰‡ + æ–‡å­—å¾ˆå°‘ + æ ‡é¢˜å«"å°é¢/cover"æˆ–æ— æ ‡é¢˜
    if images and len(text.strip()) < 100:
        if 'å°é¢' in title_lower or 'cover' in title_lower or not title_text:
            return 'cover', 'å°é¢'

    # æ£€æµ‹åªæœ‰å›¾ç‰‡å‡ ä¹æ²¡æœ‰æ–‡å­—çš„é¡µé¢ - è·³è¿‡
    if images and len(text.strip()) < 30 and not title_text:
        return 'skip', ''

    # æ£€æµ‹æ‰‰é¡µï¼ˆTitle Pageï¼‰- åŒ…å«ä¹¦åå’Œä½œè€…ä½†ä¸æ˜¯æ­£æ–‡
    if images and len(text.strip()) < 150:
        if re.search(r'\bby\s+[A-Z][a-z]+', text):
            return 'skip', ''

    # æ£€æµ‹æ ‡é¢˜ä¸ºå‰è¨€/å°é¢ç­‰çš„é¡µé¢
    skip_title_keywords = ['å°é¢', 'cover', 'æ‰‰é¡µ', 'title page']
    frontmatter_title_keywords = ['å‰è¨€', 'ç‰ˆæƒ', 'åºè¨€', 'preface', 'copyright', 'foreword', 'dedication']

    if any(keyword in title_lower for keyword in skip_title_keywords):
        return 'skip', ''

    if any(keyword in title_lower for keyword in frontmatter_title_keywords):
        return 'frontmatter', 'å‰è¨€'

    # æ£€æµ‹ç›®å½•
    if any(keyword in text_lower for keyword in ['table of contents', 'contents', 'ç›®å½•']):
        return 'toc', 'ç›®å½•'

    # æ£€æµ‹æ¨èè¯­/è¯»è€…è¯„ä»·
    testimonial_keywords = [
        "what kids have to say",
        "here's what kids",
        "teachers and librarians love",
        "what readers are saying",
        "praise for",
        "reviews",
        "è¯»è€…è¯„ä»·",
        "æ¨èè¯­",
        "i love your books",
        "best author",
        "imagination like no other"
    ]
    if any(keyword in text_lower for keyword in testimonial_keywords):
        return 'testimonial', 'æ¨èè¯­'

    # æ£€æµ‹é™„åŠ å†…å®¹ï¼ˆbonusã€podcastã€è¡¥å……ææ–™ç­‰ï¼‰
    bonus_keywords = [
        'podcast',
        'bonus chapter',
        'bonus content',
        'extra chapter',
        'supplementary',
        'appendix',
        'about the author',
        'author note',
        'acknowledgments',
        'é™„å½•',
        'ä½œè€…è¯´æ˜',
        'è¡¥å……ææ–™'
    ]
    if any(keyword in text_lower for keyword in bonus_keywords):
        return 'skip', ''

    # æ£€æµ‹å¹¿å‘Š/æ¨å¹¿å†…å®¹
    promo_keywords = [
        'visit www.',
        'order the cd',
        'musical cd',
        'for more information',
        'www.mthmusical',
        'è®¢è´­',
        'æ›´å¤šä¿¡æ¯è¯·è®¿é—®'
    ]
    if any(keyword in text_lower for keyword in promo_keywords):
        return 'skip', ''

    # æ£€æµ‹ä¹¦ç±ç›®å½•/ç³»åˆ—å¹¿å‘Š
    booklist_keywords = [
        'magic tree house',
        '#1:',
        '#2:',
        '#3:',
        'other books in this series',
        'also available',
        'ç³»åˆ—ä¸›ä¹¦',
        'æ›´å¤šå›¾ä¹¦'
    ]
    # ä¹¦ç›®é¡µç‰¹å¾ï¼šåŒ…å«å¤šä¸ªä¹¦åç¼–å·
    book_number_count = len(re.findall(r'#\d+:', text))
    if book_number_count >= 3:
        return 'skip', ''
    if any(keyword in text_lower for keyword in booklist_keywords) and book_number_count >= 2:
        return 'skip', ''

    # æ£€æµ‹å‰è¨€/ç‰ˆæƒ/çŒ®è¾ç­‰ï¼ˆå†…å®¹æ£€æµ‹ï¼‰
    frontmatter_keywords = [
        'copyright', 'all rights reserved', 'published by',
        'dedication', 'acknowledgment', 'preface', 'foreword',
        'isbn', 'ç‰ˆæƒ', 'å‰è¨€', 'åºè¨€', 'çŒ®ç»™'
    ]
    if any(keyword in text_lower for keyword in frontmatter_keywords):
        return 'frontmatter', 'å‰è¨€'

    # æ£€æµ‹çŒ®è¾æ ¼å¼ "For xxx, who xxx" æˆ– "For xxx and xxx"
    if re.match(r'^for\s+[a-z]+(\s+and\s+[a-z]+)?,?\s+(who|with)', text_lower.strip()):
        return 'frontmatter', 'çŒ®è¾'

    # æ£€æµ‹å†…å®¹å¾ˆå°‘çš„é¡µé¢ï¼ˆå¯èƒ½æ˜¯åˆ†éš”é¡µæˆ–ç©ºç™½é¡µï¼‰
    if len(text.strip()) < 50:
        return 'skip', ''

    # å°è¯•æ£€æµ‹çœŸæ­£çš„ç« èŠ‚æ ‡é¢˜ï¼ˆå¦‚ "1 Into the Woods"ï¼‰
    if title_text:
        # åŒ¹é… "1 Title" æˆ– "Chapter 1 Title" æ ¼å¼
        chapter_num_match = re.match(r'^(\d+)\s+(.+)$', title_text)
        if chapter_num_match:
            return 'chapter', title_text

    # æ£€æµ‹ "Chapter X" æ ¼å¼
    chapter_match = re.search(r'chapter\s+(\d+)', text_lower)
    if chapter_match:
        # æ£€æŸ¥æ˜¯å¦å®é™…ä¸Šæ˜¯æ¨èè¯­å†…å®¹
        if any(keyword in text_lower for keyword in ['love your books', 'best author', 'imagination']):
            return 'testimonial', 'æ¨èè¯­'

        chapter_num = chapter_match.group(1)
        if title_text:
            return 'chapter', title_text
        return 'chapter', f'Chapter {chapter_num}'

    # é»˜è®¤å°è¯•ä»æ ‡é¢˜æ ‡ç­¾è·å–
    if title_text:
        return 'chapter', title_text

    return 'chapter', ''


def is_substantial_chapter(text: str, min_words: int = 100) -> bool:
    """
    åˆ¤æ–­æ˜¯å¦æ˜¯æœ‰å®è´¨å†…å®¹çš„ç« èŠ‚
    è¿‡æ»¤æ‰åªæœ‰ä¸€ä¸¤å¥è¯çš„ç« èŠ‚

    Args:
        text: ç« èŠ‚æ–‡æœ¬å†…å®¹
        min_words: æœ€å°‘å•è¯æ•°ï¼Œé»˜è®¤100

    Returns:
        Trueè¡¨ç¤ºæ˜¯æœ‰æ•ˆç« èŠ‚ï¼ŒFalseè¡¨ç¤ºåº”è¯¥è·³è¿‡
    """
    # æå–å•è¯
    words = re.findall(r'\b[a-zA-Z]+\b', text.lower())
    word_count = len([w for w in words if len(w) > 2])

    # æ£€æŸ¥å•è¯æ•°æ˜¯å¦è¶³å¤Ÿ
    if word_count < min_words:
        return False

    # æ£€æŸ¥å¥å­æ•°ï¼ˆé€šè¿‡å¥å·ã€é—®å·ã€æ„Ÿå¹å·è®¡æ•°ï¼‰
    sentences = re.split(r'[.!?]+', text)
    sentence_count = len([s for s in sentences if s.strip() and len(s.strip()) > 10])

    # è‡³å°‘éœ€è¦3ä¸ªæœ‰æ•ˆå¥å­
    if sentence_count < 3:
        return False

    return True


def normalize_epub_path(path: str) -> str:
    """å½’ä¸€åŒ–EPUBå†…éƒ¨è·¯å¾„"""
    return path.replace('\\', '/')


def resolve_opf_path(opf_dir: Optional[str], href: str) -> str:
    """æ ¹æ®OPFç›®å½•è§£æç›¸å¯¹è·¯å¾„"""
    normalized = normalize_epub_path(href)
    if not opf_dir:
        return normalized.lstrip('./')
    combined = normalize_epub_path(os.path.join(opf_dir, normalized))
    return combined.lstrip('./')


def load_opf_data(epub_path: str) -> tuple[Optional[ET.Element], Optional[str], list[ET.Element]]:
    """è§£æEPUBçš„OPFæ–‡ä»¶ï¼Œè¿”å›rootã€ç›®å½•å’Œmanifestæ¡ç›®"""
    try:
        with zipfile.ZipFile(epub_path) as zf:
            container_xml = zf.read('META-INF/container.xml')
            container_root = ET.fromstring(container_xml)
            rootfile_el = container_root.find('.//{*}rootfile')
            if rootfile_el is None:
                logger.warning("æœªæ‰¾åˆ°rootfileï¼Œæ— æ³•è§£æOPF")
                return None, None, []
            opf_path = rootfile_el.get('full-path')
            opf_data = zf.read(opf_path)
            opf_root = ET.fromstring(opf_data)
            manifest_items = opf_root.findall('.//{*}manifest/{*}item')
            opf_dir = os.path.dirname(opf_path)
            return opf_root, opf_dir, manifest_items
    except Exception as e:
        logger.warning(f"è§£æOPFå¤±è´¥: {e}")
        return None, None, []


def build_manifest_map(manifest_items: list[ET.Element]) -> Dict[str, str]:
    """æ„å»ºmanifestä¸­idåˆ°hrefçš„æ˜ å°„"""
    manifest_map: Dict[str, str] = {}
    for item in manifest_items:
        item_id = item.get('id')
        href = item.get('href')
        if item_id and href:
            manifest_map[item_id] = href
    return manifest_map


def find_manifest_cover_href(manifest_items: list[ET.Element]) -> Optional[str]:
    """åœ¨manifestä¸­æŸ¥æ‰¾propertiesåŒ…å«cover-imageçš„å…ƒç´ """
    for item in manifest_items:
        props = (item.get('properties') or '').lower()
        if 'cover-image' in props:
            return item.get('href')
    return None


def find_guide_cover_hrefs(opf_root: Optional[ET.Element]) -> list[str]:
    """è§£æguideåŒºåŸŸä¸­çš„coverå¼•ç”¨"""
    if opf_root is None:
        return []
    hrefs = []
    for ref in opf_root.findall('.//{*}reference'):
        if (ref.get('type') or '').lower() == 'cover':
            href = ref.get('href')
            if href:
                hrefs.append(href)
    return hrefs


def extract_guide_image_references(epub_path: str, opf_dir: Optional[str], guide_hrefs: list[str]) -> tuple[Set[str], Set[str]]:
    """ä»guideå¼•ç”¨çš„æ–‡æ¡£ä¸­æå–å›¾ç‰‡è·¯å¾„ä¸æ–‡ä»¶å"""
    normalized_paths: Set[str] = set()
    basenames: Set[str] = set()
    if not guide_hrefs:
        return normalized_paths, basenames

    try:
        with zipfile.ZipFile(epub_path) as zf:
            for href in guide_hrefs:
                resolved_doc = resolve_opf_path(opf_dir, href)
                if resolved_doc not in zf.namelist():
                    continue
                doc_data = zf.read(resolved_doc)
                soup = BeautifulSoup(doc_data, 'html.parser')
                doc_dir = os.path.dirname(href)
                for img in soup.find_all('img'):
                    src = img.get('src')
                    if not src:
                        continue
                    img_path = os.path.normpath(os.path.join(doc_dir, src))
                    normalized = resolve_opf_path(opf_dir, img_path)
                    normalized_paths.add(normalize_epub_path(normalized))
                    basenames.add(os.path.basename(normalized).lower())
    except Exception as e:
        logger.warning(f"è§£æguideå°é¢å¼•ç”¨å¤±è´¥: {e}")

    return normalized_paths, basenames


def is_cover_filename(filename: str) -> bool:
    """ä¾æ®å¸¸è§å‘½ååˆ¤æ–­æ˜¯å¦å¯èƒ½æ˜¯å°é¢"""
    name_lower = filename.lower()
    cover_patterns = [
        'cover.jpg', 'cover.jpeg', 'cover.png', 'cover.gif',
        'cover-image', 'coverimage', '/cover.', '_cover.', '-cover.',
    ]
    return any(pattern in name_lower for pattern in cover_patterns)


def matches_href(item_name: str, target_href: str, opf_dir: Optional[str]) -> bool:
    """åˆ¤æ–­å›¾ç‰‡è·¯å¾„æ˜¯å¦åŒ¹é…manifestä¸­çš„href"""
    if not target_href:
        return False
    normalized_item = normalize_epub_path(item_name)
    normalized_target = normalize_epub_path(target_href)
    resolved_target = resolve_opf_path(opf_dir, normalized_target)
    return (
        normalized_item == normalized_target
        or normalized_item == resolved_target
        or normalized_item.endswith(normalized_target)
    )
def extract_real_chapter_number(text: str) -> int:
    """ä»æ–‡æœ¬ä¸­æå–çœŸæ­£çš„ç« èŠ‚ç¼–å·"""
    # åŒ¹é… "Chapter X" æˆ– "ç¬¬Xç« " æ ¼å¼
    match = re.search(r'chapter\s+(\d+)', text.lower())
    if match:
        return int(match.group(1))

    match = re.search(r'ç¬¬\s*(\d+)\s*ç« ', text)
    if match:
        return int(match.group(1))

    return 0


def import_epub(epub_path: str, level: str = None, lexile: str = None, series: str = None, category: str = None) -> str:
    """
    å¯¼å…¥ EPUB æ–‡ä»¶åˆ°æ•°æ®åº“

    Args:
        epub_path: EPUBæ–‡ä»¶è·¯å¾„
        level: éš¾åº¦ç­‰çº§ï¼ˆä¿ç•™ä»¥å…¼å®¹æ—§ä»£ç ï¼‰
        lexile: è“æ€å€¼ï¼ˆå¦‚"530L"ï¼‰
        series: ç³»åˆ—åï¼ˆå¦‚"Magic Tree House"ï¼‰
        category: åˆ†ç±»ï¼ˆ'fiction'æˆ–'non-fiction'ï¼‰

    Returns:
        ä¹¦ç±ID
    """
    if not os.path.exists(epub_path):
        raise FileNotFoundError(f"EPUB file not found: {epub_path}")

    # è¯»å– EPUB
    book = epub.read_epub(epub_path)

    # è·å–å…ƒæ•°æ®
    title = book.get_metadata('DC', 'title')
    title = title[0][0] if title else os.path.basename(epub_path).replace('.epub', '')

    author = book.get_metadata('DC', 'creator')
    author = author[0][0] if author else "Unknown"

    description = book.get_metadata('DC', 'description')
    description = description[0][0] if description else ""

    # ç”Ÿæˆä¹¦ç± ID
    book_id = str(uuid.uuid4())

    # ä» EPUB ç›®å½•ä¸­æå–ç« èŠ‚æ ‡é¢˜æ˜ å°„
    toc_titles = {}  # æ–‡ä»¶å -> æ ‡é¢˜
    for item in book.toc:
        if isinstance(item, epub.Link):
            # æå–æ–‡ä»¶åï¼ˆä¸å«é”šç‚¹ï¼‰
            href = item.href.split('#')[0]
            toc_titles[href] = item.title
        elif isinstance(item, tuple):
            # åµŒå¥—ç›®å½•ç»“æ„
            section, links = item
            for link in links:
                if isinstance(link, epub.Link):
                    href = link.href.split('#')[0]
                    toc_titles[href] = link.title

    # åˆ›å»ºä¹¦ç±å›¾ç‰‡ç›®å½•ï¼ˆæœ¬åœ°å­˜å‚¨fallbackï¼‰
    backend_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    images_dir = os.path.join(backend_dir, "data", "images", book_id)

    # è§£æOPFä»¥è¾…åŠ©å°é¢æå–
    opf_root, opf_dir, manifest_items = load_opf_data(epub_path)
    manifest_map = build_manifest_map(manifest_items)
    manifest_cover_href = find_manifest_cover_href(manifest_items)
    guide_cover_hrefs = find_guide_cover_hrefs(opf_root)
    guide_image_paths, guide_image_basenames = extract_guide_image_references(
        epub_path, opf_dir, guide_cover_hrefs
    )

    # æŸ¥æ‰¾å°é¢å›¾ç‰‡IDï¼ˆä»metadataä¸­ï¼‰
    cover_image_id = None
    for meta in book.get_metadata('OPF', 'cover'):
        if meta:
            cover_image_id = meta[0]
            logger.info(f"ä»metadataæ‰¾åˆ°å°é¢ID: {cover_image_id}")
            break

    # æå–å¹¶ä¿å­˜æ‰€æœ‰å›¾ç‰‡ï¼Œå»ºç«‹æ˜ å°„å…³ç³»
    image_map = {}  # åŸå§‹è·¯å¾„ -> æ–°URL
    cover_path = None
    cover_candidates: list[tuple[int, str, str, str]] = []  # (ä¼˜å…ˆçº§, url, æ–‡ä»¶å, æè¿°)

    def add_cover_candidate(priority: int, url: str, name: str, reason: str):
        cover_candidates.append((priority, url, name, reason))
        logger.info(f"ğŸ“Œ æ‰¾åˆ°å°é¢å€™é€‰ï¼ˆ{reason}ï¼‰: {name}")

    for item in book.get_items():
        if item.get_type() == ebooklib.ITEM_IMAGE:
            # è·å–å›¾ç‰‡æ–‡ä»¶å
            item_name = item.get_name()
            normalized_name = normalize_epub_path(item_name)
            file_name = os.path.basename(item_name)

            # ç”Ÿæˆå”¯ä¸€æ–‡ä»¶åé¿å…å†²çª
            unique_name = f"{uuid.uuid4().hex[:8]}_{file_name}"
            image_data = item.get_content()

            # å°è¯•ä¸Šä¼ åˆ°OSSï¼Œå¤±è´¥åˆ™ä½¿ç”¨æœ¬åœ°å­˜å‚¨
            try:
                if oss_helper.enabled:
                    # ä¸Šä¼ åˆ°OSS
                    object_name = f"{book_id}/{unique_name}"
                    new_url = oss_helper.upload_image(image_data, object_name)
                    logger.info(f"å›¾ç‰‡å·²ä¸Šä¼ åˆ°OSS: {object_name}")
                else:
                    # ä½¿ç”¨æœ¬åœ°å­˜å‚¨
                    os.makedirs(images_dir, exist_ok=True)
                    save_path = os.path.join(images_dir, unique_name)
                    new_url = oss_helper.save_image_local(image_data, save_path)
                    logger.info(f"å›¾ç‰‡å·²ä¿å­˜åˆ°æœ¬åœ°: {save_path}")

            except Exception as e:
                # OSSä¸Šä¼ å¤±è´¥ï¼Œfallbackåˆ°æœ¬åœ°å­˜å‚¨
                logger.warning(f"OSSä¸Šä¼ å¤±è´¥ï¼Œä½¿ç”¨æœ¬åœ°å­˜å‚¨: {e}")
                os.makedirs(images_dir, exist_ok=True)
                save_path = os.path.join(images_dir, unique_name)
                new_url = oss_helper.save_image_local(image_data, save_path)

            # å»ºç«‹æ˜ å°„ï¼šå„ç§å¯èƒ½çš„å¼•ç”¨è·¯å¾„ -> æ–°URL
            image_map[item_name] = new_url
            image_map[file_name] = new_url
            image_map[os.path.basename(item_name)] = new_url

            # ç›¸å¯¹è·¯å¾„å˜ä½“
            if '/' in item_name:
                image_map['../' + item_name] = new_url
                image_map['./' + item_name] = new_url

            # æ£€æŸ¥æ˜¯å¦ä¸ºå°é¢å›¾ç‰‡
            if cover_path:
                continue

            # æ–¹æ³•1ï¼šæ£€æŸ¥æ˜¯å¦åŒ¹é…metadataä¸­çš„cover ID
            if cover_image_id:
                metadata_match = (
                    item.get_id() == cover_image_id or
                    file_name == cover_image_id or
                    normalized_name == normalize_epub_path(cover_image_id) or
                    normalized_name.endswith(normalize_epub_path(cover_image_id))
                )
                if not metadata_match and cover_image_id in manifest_map:
                    metadata_match = matches_href(normalized_name, manifest_map[cover_image_id], opf_dir)
                if metadata_match:
                    cover_path = new_url
                    logger.info(f"âœ… æ‰¾åˆ°å°é¢å›¾ç‰‡ï¼ˆmetadataï¼‰: {file_name}")
                    continue

            # æ–¹æ³•2ï¼šmanifestå±æ€§properties="cover-image"
            if manifest_cover_href and matches_href(normalized_name, manifest_cover_href, opf_dir):
                cover_path = new_url
                logger.info(f"âœ… æ‰¾åˆ°å°é¢å›¾ç‰‡ï¼ˆmanifest cover-imageï¼‰: {file_name}")
                continue

            # æ–¹æ³•3ï¼šguideåŒºåŸŸæŒ‡å‘çš„å°é¢
            if guide_image_paths:
                if (normalized_name in guide_image_paths or
                        file_name.lower() in guide_image_basenames):
                    cover_path = new_url
                    logger.info(f"âœ… æ‰¾åˆ°å°é¢å›¾ç‰‡ï¼ˆguideå¼•ç”¨ï¼‰: {file_name}")
                    continue

            # æ–¹æ³•4ï¼šå¸¸è§æ–‡ä»¶å/è·¯å¾„æ¨¡å¼
            if is_cover_filename(file_name):
                add_cover_candidate(1, new_url, file_name, 'æ–‡ä»¶ååŒ¹é…')
                continue

            # æ–¹æ³•5ï¼šå•å±‚ç›®å½•çš„å›¾ç‰‡ä½œä¸ºæ¬¡çº§å€™é€‰
            if normalized_name.count('/') <= 1:
                add_cover_candidate(2, new_url, file_name, 'ç›®å½•æµ…å±‚å›¾ç‰‡')

    # å¦‚æœè¿˜æ²¡æœ‰æ‰¾åˆ°å°é¢ï¼Œä»å€™é€‰åˆ—è¡¨ä¸­é€‰æ‹©ä¼˜å…ˆçº§æœ€é«˜çš„
    if not cover_path and cover_candidates:
        cover_candidates.sort(key=lambda x: x[0])
        priority, url, name, reason = cover_candidates[0]
        cover_path = url
        logger.info(f"âœ… é€‰æ‹©å°é¢ï¼ˆå€™é€‰: {reason}ï¼‰: {name}")

    # å¦‚æœä»ç„¶æ²¡æœ‰å°é¢ï¼Œä½¿ç”¨ç¬¬ä¸€å¼ å›¾ç‰‡ï¼ˆfallbackï¼‰
    if not cover_path and image_map:
        cover_path = list(image_map.values())[0]
        logger.warning(f"âš ï¸  ä½¿ç”¨ç¬¬ä¸€å¼ å›¾ç‰‡ä½œä¸ºå°é¢ï¼ˆfallbackï¼‰")

    # æå–ç« èŠ‚å†…å®¹
    chapters_data = []
    all_words = []

    for item in book.get_items():
        if item.get_type() == ebooklib.ITEM_DOCUMENT:
            content = item.get_content().decode('utf-8', errors='ignore')
            item_name = item.get_name()  # è·å–æ–‡æ¡£æ–‡ä»¶å

            # æ›¿æ¢å›¾ç‰‡è·¯å¾„
            soup = BeautifulSoup(content, 'html.parser')

            for img in soup.find_all('img'):
                src = img.get('src', '')
                # å°è¯•å¤šç§åŒ¹é…æ–¹å¼
                new_src = None
                for old_path, new_path in image_map.items():
                    if old_path in src or src.endswith(old_path) or os.path.basename(src) == os.path.basename(old_path):
                        new_src = new_path
                        break
                if new_src:
                    img['src'] = new_src

            # ä¹Ÿå¤„ç† image æ ‡ç­¾ (SVG ä¸­å¯èƒ½ç”¨åˆ°)
            for img in soup.find_all('image'):
                href = img.get('xlink:href', '') or img.get('href', '')
                for old_path, new_path in image_map.items():
                    if old_path in href or href.endswith(old_path):
                        if img.get('xlink:href'):
                            img['xlink:href'] = new_path
                        if img.get('href'):
                            img['href'] = new_path
                        break

            content = str(soup)

            # æå–æ–‡æœ¬ç”¨äºåˆ†æ
            text = extract_text_from_html(content)
            words = extract_words(text)
            all_words.extend(words)

            # æ£€æµ‹ç« èŠ‚ç±»å‹å’Œæ ‡é¢˜
            chapter_type, detected_title = detect_chapter_type(soup, text)

            # åªä¿ç•™æ­£æ–‡ç« èŠ‚ï¼Œè·³è¿‡æ‰€æœ‰å‰ç½®å†…å®¹
            if chapter_type in ('skip', 'cover', 'toc', 'frontmatter', 'testimonial'):
                continue

            # ä¼˜å…ˆä» TOC è·å–æ ‡é¢˜
            toc_title = toc_titles.get(item_name, '')
            if toc_title:
                # æ¸…ç†æ ‡é¢˜ä¸­çš„ç« èŠ‚ç¼–å·å‰ç¼€ï¼ˆå¦‚ "1. Into the Woods" -> "Into the Woods"ï¼‰
                cleaned_title = re.sub(r'^(\d+)\.\s*', '', toc_title)
                detected_title = cleaned_title if cleaned_title else toc_title

            # æå–çœŸæ­£çš„ç« èŠ‚ç¼–å·
            real_chapter_num = extract_real_chapter_number(text)

            # æ­£æ–‡ç« èŠ‚
            if real_chapter_num > 0:
                display_chapter_num = real_chapter_num
            else:
                # ä»æ ‡é¢˜ä¸­æå–æ•°å­—ï¼ˆå¦‚ "1 Into the Woods"ï¼‰
                title_num_match = re.match(r'^(\d+)\s+', detected_title)
                if title_num_match:
                    display_chapter_num = int(title_num_match.group(1))
                else:
                    display_chapter_num = len(chapters_data) + 1

            # å¦‚æœæ²¡æœ‰æ£€æµ‹åˆ°æ ‡é¢˜ï¼Œä½¿ç”¨é»˜è®¤å€¼
            if not detected_title:
                detected_title = f'Chapter {display_chapter_num}'

            chapters_data.append({
                'id': str(uuid.uuid4()),
                'book_id': book_id,
                'chapter_number': display_chapter_num,
                'title': detected_title,
                'content': content,
                'word_count': len(words)
            })

    # æŒ‰ç« èŠ‚ç¼–å·æ’åº
    chapters_data.sort(key=lambda x: x['chapter_number'])

    # é‡æ–°ç¼–å·ï¼šä»1å¼€å§‹è¿ç»­
    for i, chapter in enumerate(chapters_data):
        chapter['chapter_number'] = i + 1

    # ç»Ÿè®¡è¯é¢‘
    word_counts = Counter(all_words)
    total_words = len(all_words)

    # å–é«˜é¢‘è¯ï¼ˆå‡ºç°æ¬¡æ•° >= 3 ä¸”ä¸æ˜¯å¸¸è§è™šè¯ï¼‰
    common_words = {'the', 'a', 'an', 'is', 'are', 'was', 'were', 'be', 'been', 'being',
                    'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could',
                    'should', 'may', 'might', 'must', 'shall', 'can', 'need', 'dare',
                    'and', 'or', 'but', 'if', 'then', 'else', 'when', 'where', 'why',
                    'how', 'what', 'which', 'who', 'whom', 'this', 'that', 'these',
                    'those', 'for', 'with', 'about', 'against', 'between', 'into',
                    'through', 'during', 'before', 'after', 'above', 'below', 'from',
                    'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again',
                    'further', 'then', 'once', 'here', 'there', 'all', 'each', 'few',
                    'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only',
                    'own', 'same', 'so', 'than', 'too', 'very', 'just', 'also', 'now',
                    'you', 'your', 'yours', 'yourself', 'he', 'him', 'his', 'himself',
                    'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they',
                    'them', 'their', 'theirs', 'themselves', 'we', 'us', 'our', 'ours',
                    'ourselves', 'said', 'say', 'says', 'saying', 'got', 'get', 'gets',
                    'getting', 'went', 'go', 'goes', 'going', 'come', 'comes', 'coming',
                    'came', 'see', 'saw', 'seen', 'look', 'looked', 'looking', 'looks'}

    high_freq_words = [(word, count) for word, count in word_counts.most_common(200)
                       if count >= 3 and word not in common_words][:100]

    # ä¿å­˜åˆ°æ•°æ®åº“
    db = SessionLocal()
    try:
        # åˆ›å»ºä¹¦ç±è®°å½•
        db_book = Book(
            id=book_id,
            title=title,
            author=author,
            cover=cover_path,  # è®¾ç½®å°é¢
            level=level,
            lexile=lexile,  # è“æ€å€¼
            series=series,  # ç³»åˆ—å
            category=category,  # åˆ†ç±»
            word_count=total_words,
            description=description,
            epub_path=epub_path
        )
        db.add(db_book)

        # åˆ›å»ºç« èŠ‚è®°å½•
        for chapter_data in chapters_data:
            db_chapter = Chapter(**chapter_data)
            db.add(db_chapter)

        # åˆ›å»ºè¯æ±‡è®°å½•
        for word, freq in high_freq_words:
            db_vocab = BookVocabulary(
                id=str(uuid.uuid4()),
                book_id=book_id,
                word=word,
                frequency=freq
            )
            db.add(db_vocab)

        db.commit()

        # åŒæ—¶å†™å…¥Supabaseï¼ˆå¦‚æœå·²é…ç½®ï¼‰
        if supabase_client.enabled:
            try:
                logger.info("ğŸ“¤ å¼€å§‹åŒæ­¥æ•°æ®åˆ°Supabase...")

                # 1. æ’å…¥ä¹¦ç±æ•°æ®
                book_data_for_supabase = {
                    'id': book_id,
                    'title': title,
                    'author': author,
                    'cover': cover_path,
                    'level': level,
                    'lexile': lexile,  # è“æ€å€¼
                    'series': series,  # ç³»åˆ—å
                    'category': category,  # åˆ†ç±»
                    'word_count': total_words,
                    'description': description,
                    'epub_path': epub_path,
                }
                supabase_client.insert_book(book_data_for_supabase)

                # 2. æ‰¹é‡æ’å…¥ç« èŠ‚æ•°æ®
                chapters_for_supabase = []
                for chapter_data in chapters_data:
                    chapters_for_supabase.append({
                        'id': chapter_data['id'],
                        'book_id': chapter_data['book_id'],
                        'chapter_number': chapter_data['chapter_number'],
                        'title': chapter_data.get('title'),
                        'content': chapter_data['content'],
                        'word_count': chapter_data['word_count'],
                    })
                supabase_client.bulk_insert_chapters(chapters_for_supabase)

                # 3. æ‰¹é‡æ’å…¥è¯æ±‡æ•°æ®
                vocab_for_supabase = []
                for word, freq in high_freq_words:
                    vocab_for_supabase.append({
                        'id': str(uuid.uuid4()),
                        'book_id': book_id,
                        'word': word,
                        'frequency': freq,
                    })
                supabase_client.bulk_insert_vocabulary(vocab_for_supabase)

                logger.info("âœ… æ•°æ®å·²æˆåŠŸåŒæ­¥åˆ°Supabase")
            except Exception as e:
                logger.warning(f"âš ï¸ SupabaseåŒæ­¥å¤±è´¥ï¼ˆä¸å½±å“æœ¬åœ°SQLiteï¼‰: {e}")

        print(f"âœ… Successfully imported: {title}")
        print(f"   - Author: {author}")
        print(f"   - Chapters: {len(chapters_data)}")
        print(f"   - Total words: {total_words}")
        print(f"   - High-freq vocabulary: {len(high_freq_words)}")
        print(f"   - Images: {len(image_map) // 3}")  # é™¤ä»¥3å› ä¸ºæ¯å¼ å›¾æœ‰å¤šä¸ªæ˜ å°„
        print(f"   - Cover: {cover_path}")
        print(f"   - Book ID: {book_id}")

        return book_id

    except Exception as e:
        db.rollback()
        # æ¸…ç†å·²åˆ›å»ºçš„å›¾ç‰‡
        logger.error(f"å¯¼å…¥ä¹¦ç±å¤±è´¥ï¼Œæ¸…ç†å›¾ç‰‡èµ„æº: {e}")

        # æ¸…ç†OSSå›¾ç‰‡
        if oss_helper.enabled:
            oss_helper.delete_images(book_id)

        # æ¸…ç†æœ¬åœ°å›¾ç‰‡ç›®å½•
        if os.path.exists(images_dir):
            shutil.rmtree(images_dir)

        raise e
    finally:
        db.close()


def main():
    parser = argparse.ArgumentParser(description='Import EPUB book to database')
    parser.add_argument('epub_file', help='Path to EPUB file')
    parser.add_argument('--level', '-l', help='Book level (e.g., å­¦å‰, ä¸€å¹´çº§, åˆä¸€)')

    args = parser.parse_args()

    # ç¡®ä¿æ•°æ®åº“è¡¨å­˜åœ¨
    create_tables()

    # å¯¼å…¥ä¹¦ç±
    import_epub(args.epub_file, args.level)


if __name__ == '__main__':
    main()
