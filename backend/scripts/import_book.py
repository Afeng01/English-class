"""
EPUB ä¹¦ç±å¯¼å…¥è„šæœ¬
ç”¨æ³•: python import_book.py <epub_file> [--level <level>]
"""
import argparse
import os
import re
import sys
import uuid
import shutil
import logging
from collections import Counter

import ebooklib
from ebooklib import epub
from bs4 import BeautifulSoup

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from app.models.database import SessionLocal, create_tables, Book, Chapter, BookVocabulary
from app.utils.oss_helper import oss_helper
from app.utils.supabase_client import supabase_client

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def extract_text_from_html(html_content: str) -> str:
    """ä» HTML ä¸­æå–çº¯æ–‡æœ¬"""
    soup = BeautifulSoup(html_content, 'html.parser')
    return soup.get_text(separator=' ', strip=True)


def extract_words(text: str) -> list:
    """æå–æ–‡æœ¬ä¸­çš„å•è¯"""
    # åªä¿ç•™è‹±æ–‡å•è¯ï¼Œè½¬å°å†™
    words = re.findall(r'\b[a-zA-Z]+\b', text.lower())
    # è¿‡æ»¤æ‰å¤ªçŸ­çš„è¯
    return [w for w in words if len(w) > 2]


def detect_chapter_type(soup, text: str) -> tuple[str, str]:
    """
    æ£€æµ‹ç« èŠ‚ç±»å‹å¹¶æå–æ ‡é¢˜
    è¿”å›: (chapter_type, title)
    chapter_type: 'cover', 'toc', 'frontmatter', 'chapter', 'testimonial', 'skip'
    """
    # æ ‡å‡†åŒ–å¼•å·å’Œå…¶ä»–ç‰¹æ®Šå­—ç¬¦
    text_normalized = text.replace(''', "'").replace(''', "'").replace('"', '"').replace('"', '"')
    text_lower = text_normalized.lower()
    images = soup.find_all('img')

    # å…ˆè·å–æ ‡é¢˜æ ‡ç­¾å†…å®¹
    title_tag = soup.find(['h1', 'h2', 'h3'])
    title_text = title_tag.get_text(strip=True) if title_tag else ''
    title_lower = title_text.lower()

    # æ£€æµ‹å°é¢ï¼šæœ‰å›¾ç‰‡ + æ–‡å­—å¾ˆå°‘ + æ ‡é¢˜å«"å°é¢/cover"æˆ–æ— æ ‡é¢˜
    if images and len(text.strip()) < 100:
        if 'å°é¢' in title_lower or 'cover' in title_lower or not title_text:
            return 'cover', 'å°é¢'

    # æ£€æµ‹åªæœ‰å›¾ç‰‡å‡ ä¹æ²¡æœ‰æ–‡å­—çš„é¡µé¢ - è·³è¿‡
    if images and len(text.strip()) < 30 and not title_text:
        return 'skip', ''

    # æ£€æµ‹æ‰‰é¡µï¼ˆTitle Pageï¼‰- åŒ…å«ä¹¦åå’Œä½œè€…ä½†ä¸æ˜¯æ­£æ–‡
    if images and len(text.strip()) < 150:
        if re.search(r'\bby\s+[A-Z][a-z]+', text):
            return 'skip', ''

    # æ£€æµ‹æ ‡é¢˜ä¸ºå‰è¨€/å°é¢ç­‰çš„é¡µé¢
    skip_title_keywords = ['å°é¢', 'cover', 'æ‰‰é¡µ', 'title page']
    frontmatter_title_keywords = ['å‰è¨€', 'ç‰ˆæƒ', 'åºè¨€', 'preface', 'copyright', 'foreword', 'dedication']

    if any(keyword in title_lower for keyword in skip_title_keywords):
        return 'skip', ''

    if any(keyword in title_lower for keyword in frontmatter_title_keywords):
        return 'frontmatter', 'å‰è¨€'

    # æ£€æµ‹ç›®å½•
    if any(keyword in text_lower for keyword in ['table of contents', 'contents', 'ç›®å½•']):
        return 'toc', 'ç›®å½•'

    # æ£€æµ‹æ¨èè¯­/è¯»è€…è¯„ä»·
    testimonial_keywords = [
        "what kids have to say",
        "here's what kids",
        "teachers and librarians love",
        "what readers are saying",
        "praise for",
        "reviews",
        "è¯»è€…è¯„ä»·",
        "æ¨èè¯­",
        "i love your books",
        "best author",
        "imagination like no other"
    ]
    if any(keyword in text_lower for keyword in testimonial_keywords):
        return 'testimonial', 'æ¨èè¯­'

    # æ£€æµ‹é™„åŠ å†…å®¹ï¼ˆbonusã€podcastã€è¡¥å……ææ–™ç­‰ï¼‰
    bonus_keywords = [
        'podcast',
        'bonus chapter',
        'bonus content',
        'extra chapter',
        'supplementary',
        'appendix',
        'about the author',
        'author note',
        'acknowledgments',
        'é™„å½•',
        'ä½œè€…è¯´æ˜',
        'è¡¥å……ææ–™'
    ]
    if any(keyword in text_lower for keyword in bonus_keywords):
        return 'skip', ''

    # æ£€æµ‹å¹¿å‘Š/æ¨å¹¿å†…å®¹
    promo_keywords = [
        'visit www.',
        'order the cd',
        'musical cd',
        'for more information',
        'www.mthmusical',
        'è®¢è´­',
        'æ›´å¤šä¿¡æ¯è¯·è®¿é—®'
    ]
    if any(keyword in text_lower for keyword in promo_keywords):
        return 'skip', ''

    # æ£€æµ‹ä¹¦ç±ç›®å½•/ç³»åˆ—å¹¿å‘Š
    booklist_keywords = [
        'magic tree house',
        '#1:',
        '#2:',
        '#3:',
        'other books in this series',
        'also available',
        'ç³»åˆ—ä¸›ä¹¦',
        'æ›´å¤šå›¾ä¹¦'
    ]
    # ä¹¦ç›®é¡µç‰¹å¾ï¼šåŒ…å«å¤šä¸ªä¹¦åç¼–å·
    book_number_count = len(re.findall(r'#\d+:', text))
    if book_number_count >= 3:
        return 'skip', ''
    if any(keyword in text_lower for keyword in booklist_keywords) and book_number_count >= 2:
        return 'skip', ''

    # æ£€æµ‹å‰è¨€/ç‰ˆæƒ/çŒ®è¾ç­‰ï¼ˆå†…å®¹æ£€æµ‹ï¼‰
    frontmatter_keywords = [
        'copyright', 'all rights reserved', 'published by',
        'dedication', 'acknowledgment', 'preface', 'foreword',
        'isbn', 'ç‰ˆæƒ', 'å‰è¨€', 'åºè¨€', 'çŒ®ç»™'
    ]
    if any(keyword in text_lower for keyword in frontmatter_keywords):
        return 'frontmatter', 'å‰è¨€'

    # æ£€æµ‹çŒ®è¾æ ¼å¼ "For xxx, who xxx" æˆ– "For xxx and xxx"
    if re.match(r'^for\s+[a-z]+(\s+and\s+[a-z]+)?,?\s+(who|with)', text_lower.strip()):
        return 'frontmatter', 'çŒ®è¾'

    # æ£€æµ‹å†…å®¹å¾ˆå°‘çš„é¡µé¢ï¼ˆå¯èƒ½æ˜¯åˆ†éš”é¡µæˆ–ç©ºç™½é¡µï¼‰
    if len(text.strip()) < 50:
        return 'skip', ''

    # å°è¯•æ£€æµ‹çœŸæ­£çš„ç« èŠ‚æ ‡é¢˜ï¼ˆå¦‚ "1 Into the Woods"ï¼‰
    if title_text:
        # åŒ¹é… "1 Title" æˆ– "Chapter 1 Title" æ ¼å¼
        chapter_num_match = re.match(r'^(\d+)\s+(.+)$', title_text)
        if chapter_num_match:
            return 'chapter', title_text

    # æ£€æµ‹ "Chapter X" æ ¼å¼
    chapter_match = re.search(r'chapter\s+(\d+)', text_lower)
    if chapter_match:
        # æ£€æŸ¥æ˜¯å¦å®é™…ä¸Šæ˜¯æ¨èè¯­å†…å®¹
        if any(keyword in text_lower for keyword in ['love your books', 'best author', 'imagination']):
            return 'testimonial', 'æ¨èè¯­'

        chapter_num = chapter_match.group(1)
        if title_text:
            return 'chapter', title_text
        return 'chapter', f'Chapter {chapter_num}'

    # é»˜è®¤å°è¯•ä»æ ‡é¢˜æ ‡ç­¾è·å–
    if title_text:
        return 'chapter', title_text

    return 'chapter', ''


def is_substantial_chapter(text: str, min_words: int = 100) -> bool:
    """
    åˆ¤æ–­æ˜¯å¦æ˜¯æœ‰å®è´¨å†…å®¹çš„ç« èŠ‚
    è¿‡æ»¤æ‰åªæœ‰ä¸€ä¸¤å¥è¯çš„ç« èŠ‚

    Args:
        text: ç« èŠ‚æ–‡æœ¬å†…å®¹
        min_words: æœ€å°‘å•è¯æ•°ï¼Œé»˜è®¤100

    Returns:
        Trueè¡¨ç¤ºæ˜¯æœ‰æ•ˆç« èŠ‚ï¼ŒFalseè¡¨ç¤ºåº”è¯¥è·³è¿‡
    """
    # æå–å•è¯
    words = re.findall(r'\b[a-zA-Z]+\b', text.lower())
    word_count = len([w for w in words if len(w) > 2])

    # æ£€æŸ¥å•è¯æ•°æ˜¯å¦è¶³å¤Ÿ
    if word_count < min_words:
        return False

    # æ£€æŸ¥å¥å­æ•°ï¼ˆé€šè¿‡å¥å·ã€é—®å·ã€æ„Ÿå¹å·è®¡æ•°ï¼‰
    sentences = re.split(r'[.!?]+', text)
    sentence_count = len([s for s in sentences if s.strip() and len(s.strip()) > 10])

    # è‡³å°‘éœ€è¦3ä¸ªæœ‰æ•ˆå¥å­
    if sentence_count < 3:
        return False

    return True


def extract_real_chapter_number(text: str) -> int:
    """ä»æ–‡æœ¬ä¸­æå–çœŸæ­£çš„ç« èŠ‚ç¼–å·"""
    # åŒ¹é… "Chapter X" æˆ– "ç¬¬Xç« " æ ¼å¼
    match = re.search(r'chapter\s+(\d+)', text.lower())
    if match:
        return int(match.group(1))

    match = re.search(r'ç¬¬\s*(\d+)\s*ç« ', text)
    if match:
        return int(match.group(1))

    return 0


def import_epub(epub_path: str, level: str = None) -> str:
    """
    å¯¼å…¥ EPUB æ–‡ä»¶åˆ°æ•°æ®åº“
    è¿”å›ä¹¦ç± ID
    """
    if not os.path.exists(epub_path):
        raise FileNotFoundError(f"EPUB file not found: {epub_path}")

    # è¯»å– EPUB
    book = epub.read_epub(epub_path)

    # è·å–å…ƒæ•°æ®
    title = book.get_metadata('DC', 'title')
    title = title[0][0] if title else os.path.basename(epub_path).replace('.epub', '')

    author = book.get_metadata('DC', 'creator')
    author = author[0][0] if author else "Unknown"

    description = book.get_metadata('DC', 'description')
    description = description[0][0] if description else ""

    # ç”Ÿæˆä¹¦ç± ID
    book_id = str(uuid.uuid4())

    # ä» EPUB ç›®å½•ä¸­æå–ç« èŠ‚æ ‡é¢˜æ˜ å°„
    toc_titles = {}  # æ–‡ä»¶å -> æ ‡é¢˜
    for item in book.toc:
        if isinstance(item, epub.Link):
            # æå–æ–‡ä»¶åï¼ˆä¸å«é”šç‚¹ï¼‰
            href = item.href.split('#')[0]
            toc_titles[href] = item.title
        elif isinstance(item, tuple):
            # åµŒå¥—ç›®å½•ç»“æ„
            section, links = item
            for link in links:
                if isinstance(link, epub.Link):
                    href = link.href.split('#')[0]
                    toc_titles[href] = link.title

    # åˆ›å»ºä¹¦ç±å›¾ç‰‡ç›®å½•ï¼ˆæœ¬åœ°å­˜å‚¨fallbackï¼‰
    backend_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    images_dir = os.path.join(backend_dir, "data", "images", book_id)

    # æŸ¥æ‰¾å°é¢å›¾ç‰‡IDï¼ˆä»metadataä¸­ï¼‰
    cover_image_id = None
    for meta in book.get_metadata('OPF', 'cover'):
        if meta:
            cover_image_id = meta[0]
            logger.info(f"ä»metadataæ‰¾åˆ°å°é¢ID: {cover_image_id}")
            break

    # æå–å¹¶ä¿å­˜æ‰€æœ‰å›¾ç‰‡ï¼Œå»ºç«‹æ˜ å°„å…³ç³»
    image_map = {}  # åŸå§‹è·¯å¾„ -> æ–°URL
    cover_path = None
    cover_candidates = []  # å°é¢å€™é€‰åˆ—è¡¨

    for item in book.get_items():
        if item.get_type() == ebooklib.ITEM_IMAGE:
            # è·å–å›¾ç‰‡æ–‡ä»¶å
            item_name = item.get_name()
            file_name = os.path.basename(item_name)

            # ç”Ÿæˆå”¯ä¸€æ–‡ä»¶åé¿å…å†²çª
            unique_name = f"{uuid.uuid4().hex[:8]}_{file_name}"
            image_data = item.get_content()

            # å°è¯•ä¸Šä¼ åˆ°OSSï¼Œå¤±è´¥åˆ™ä½¿ç”¨æœ¬åœ°å­˜å‚¨
            try:
                if oss_helper.enabled:
                    # ä¸Šä¼ åˆ°OSS
                    object_name = f"{book_id}/{unique_name}"
                    new_url = oss_helper.upload_image(image_data, object_name)
                    logger.info(f"å›¾ç‰‡å·²ä¸Šä¼ åˆ°OSS: {object_name}")
                else:
                    # ä½¿ç”¨æœ¬åœ°å­˜å‚¨
                    os.makedirs(images_dir, exist_ok=True)
                    save_path = os.path.join(images_dir, unique_name)
                    new_url = oss_helper.save_image_local(image_data, save_path)
                    logger.info(f"å›¾ç‰‡å·²ä¿å­˜åˆ°æœ¬åœ°: {save_path}")

            except Exception as e:
                # OSSä¸Šä¼ å¤±è´¥ï¼Œfallbackåˆ°æœ¬åœ°å­˜å‚¨
                logger.warning(f"OSSä¸Šä¼ å¤±è´¥ï¼Œä½¿ç”¨æœ¬åœ°å­˜å‚¨: {e}")
                os.makedirs(images_dir, exist_ok=True)
                save_path = os.path.join(images_dir, unique_name)
                new_url = oss_helper.save_image_local(image_data, save_path)

            # å»ºç«‹æ˜ å°„ï¼šå„ç§å¯èƒ½çš„å¼•ç”¨è·¯å¾„ -> æ–°URL
            image_map[item_name] = new_url
            image_map[file_name] = new_url
            image_map[os.path.basename(item_name)] = new_url

            # ç›¸å¯¹è·¯å¾„å˜ä½“
            if '/' in item_name:
                image_map['../' + item_name] = new_url
                image_map['./' + item_name] = new_url

            # æ£€æŸ¥æ˜¯å¦ä¸ºå°é¢å›¾ç‰‡
            is_cover = False

            # æ–¹æ³•1ï¼šæ£€æŸ¥æ˜¯å¦åŒ¹é…metadataä¸­çš„cover ID
            if cover_image_id:
                # cover_image_idå¯èƒ½æ˜¯itemçš„idæˆ–æ–‡ä»¶å
                if (item.get_id() == cover_image_id or
                    file_name == cover_image_id or
                    item_name == cover_image_id or
                    item_name.endswith(cover_image_id)):
                    cover_path = new_url
                    is_cover = True
                    logger.info(f"âœ… æ‰¾åˆ°å°é¢å›¾ç‰‡ï¼ˆé€šè¿‡metadataï¼‰: {file_name}")

            # æ–¹æ³•2ï¼šæ£€æŸ¥æ–‡ä»¶åæ˜¯å¦åŒ…å«"cover"å…³é”®è¯
            if not is_cover:
                file_lower = file_name.lower()
                item_lower = item_name.lower()
                if ('cover' in file_lower or 'cover' in item_lower or
                    'cov' in file_lower):  # æœ‰äº›å‘½åä¸ºcov.jpg
                    cover_candidates.append((new_url, file_name, 1))  # ä¼˜å…ˆçº§1
                    logger.info(f"ğŸ“Œ æ‰¾åˆ°å°é¢å€™é€‰ï¼ˆæ–‡ä»¶ååŒ…å«coverï¼‰: {file_name}")
                # æ£€æŸ¥æ˜¯å¦åœ¨imagesæ ¹ç›®å½•ä¸”æ–‡ä»¶åç®€å•ï¼ˆå¦‚cover.jpg, image001.jpgï¼‰
                elif '/' not in item_name or item_name.count('/') <= 1:
                    # æ ¹ç›®å½•çš„å›¾ç‰‡æ›´å¯èƒ½æ˜¯å°é¢
                    cover_candidates.append((new_url, file_name, 2))  # ä¼˜å…ˆçº§2

    # å¦‚æœè¿˜æ²¡æœ‰æ‰¾åˆ°å°é¢ï¼Œä»å€™é€‰åˆ—è¡¨ä¸­é€‰æ‹©ä¼˜å…ˆçº§æœ€é«˜çš„
    if not cover_path and cover_candidates:
        # æŒ‰ä¼˜å…ˆçº§æ’åºï¼ˆä¼˜å…ˆçº§æ•°å­—è¶Šå°è¶Šä¼˜å…ˆï¼‰
        cover_candidates.sort(key=lambda x: x[2])
        cover_path = cover_candidates[0][0]
        logger.info(f"âœ… é€‰æ‹©å°é¢ï¼ˆä»å€™é€‰ä¸­ï¼‰: {cover_candidates[0][1]}")

    # å¦‚æœä»ç„¶æ²¡æœ‰å°é¢ï¼Œä½¿ç”¨ç¬¬ä¸€å¼ å›¾ç‰‡ï¼ˆfallbackï¼‰
    if not cover_path and image_map:
        cover_path = list(image_map.values())[0]
        logger.warning(f"âš ï¸  ä½¿ç”¨ç¬¬ä¸€å¼ å›¾ç‰‡ä½œä¸ºå°é¢ï¼ˆfallbackï¼‰")

    # æå–ç« èŠ‚å†…å®¹
    chapters_data = []
    all_words = []

    for item in book.get_items():
        if item.get_type() == ebooklib.ITEM_DOCUMENT:
            content = item.get_content().decode('utf-8', errors='ignore')
            item_name = item.get_name()  # è·å–æ–‡æ¡£æ–‡ä»¶å

            # æ›¿æ¢å›¾ç‰‡è·¯å¾„
            soup = BeautifulSoup(content, 'html.parser')

            for img in soup.find_all('img'):
                src = img.get('src', '')
                # å°è¯•å¤šç§åŒ¹é…æ–¹å¼
                new_src = None
                for old_path, new_path in image_map.items():
                    if old_path in src or src.endswith(old_path) or os.path.basename(src) == os.path.basename(old_path):
                        new_src = new_path
                        break
                if new_src:
                    img['src'] = new_src

            # ä¹Ÿå¤„ç† image æ ‡ç­¾ (SVG ä¸­å¯èƒ½ç”¨åˆ°)
            for img in soup.find_all('image'):
                href = img.get('xlink:href', '') or img.get('href', '')
                for old_path, new_path in image_map.items():
                    if old_path in href or href.endswith(old_path):
                        if img.get('xlink:href'):
                            img['xlink:href'] = new_path
                        if img.get('href'):
                            img['href'] = new_path
                        break

            content = str(soup)

            # æå–æ–‡æœ¬ç”¨äºåˆ†æ
            text = extract_text_from_html(content)
            words = extract_words(text)
            all_words.extend(words)

            # æ£€æµ‹ç« èŠ‚ç±»å‹å’Œæ ‡é¢˜
            chapter_type, detected_title = detect_chapter_type(soup, text)

            # åªä¿ç•™æ­£æ–‡ç« èŠ‚ï¼Œè·³è¿‡æ‰€æœ‰å‰ç½®å†…å®¹
            if chapter_type in ('skip', 'cover', 'toc', 'frontmatter', 'testimonial'):
                continue

            # ä¼˜å…ˆä» TOC è·å–æ ‡é¢˜
            toc_title = toc_titles.get(item_name, '')
            if toc_title:
                # æ¸…ç†æ ‡é¢˜ä¸­çš„ç« èŠ‚ç¼–å·å‰ç¼€ï¼ˆå¦‚ "1. Into the Woods" -> "Into the Woods"ï¼‰
                cleaned_title = re.sub(r'^(\d+)\.\s*', '', toc_title)
                detected_title = cleaned_title if cleaned_title else toc_title

            # æå–çœŸæ­£çš„ç« èŠ‚ç¼–å·
            real_chapter_num = extract_real_chapter_number(text)

            # æ­£æ–‡ç« èŠ‚
            if real_chapter_num > 0:
                display_chapter_num = real_chapter_num
            else:
                # ä»æ ‡é¢˜ä¸­æå–æ•°å­—ï¼ˆå¦‚ "1 Into the Woods"ï¼‰
                title_num_match = re.match(r'^(\d+)\s+', detected_title)
                if title_num_match:
                    display_chapter_num = int(title_num_match.group(1))
                else:
                    display_chapter_num = len(chapters_data) + 1

            # å¦‚æœæ²¡æœ‰æ£€æµ‹åˆ°æ ‡é¢˜ï¼Œä½¿ç”¨é»˜è®¤å€¼
            if not detected_title:
                detected_title = f'Chapter {display_chapter_num}'

            chapters_data.append({
                'id': str(uuid.uuid4()),
                'book_id': book_id,
                'chapter_number': display_chapter_num,
                'title': detected_title,
                'content': content,
                'word_count': len(words)
            })

    # æŒ‰ç« èŠ‚ç¼–å·æ’åº
    chapters_data.sort(key=lambda x: x['chapter_number'])

    # é‡æ–°ç¼–å·ï¼šä»1å¼€å§‹è¿ç»­
    for i, chapter in enumerate(chapters_data):
        chapter['chapter_number'] = i + 1

    # ç»Ÿè®¡è¯é¢‘
    word_counts = Counter(all_words)
    total_words = len(all_words)

    # å–é«˜é¢‘è¯ï¼ˆå‡ºç°æ¬¡æ•° >= 3 ä¸”ä¸æ˜¯å¸¸è§è™šè¯ï¼‰
    common_words = {'the', 'a', 'an', 'is', 'are', 'was', 'were', 'be', 'been', 'being',
                    'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could',
                    'should', 'may', 'might', 'must', 'shall', 'can', 'need', 'dare',
                    'and', 'or', 'but', 'if', 'then', 'else', 'when', 'where', 'why',
                    'how', 'what', 'which', 'who', 'whom', 'this', 'that', 'these',
                    'those', 'for', 'with', 'about', 'against', 'between', 'into',
                    'through', 'during', 'before', 'after', 'above', 'below', 'from',
                    'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again',
                    'further', 'then', 'once', 'here', 'there', 'all', 'each', 'few',
                    'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only',
                    'own', 'same', 'so', 'than', 'too', 'very', 'just', 'also', 'now',
                    'you', 'your', 'yours', 'yourself', 'he', 'him', 'his', 'himself',
                    'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they',
                    'them', 'their', 'theirs', 'themselves', 'we', 'us', 'our', 'ours',
                    'ourselves', 'said', 'say', 'says', 'saying', 'got', 'get', 'gets',
                    'getting', 'went', 'go', 'goes', 'going', 'come', 'comes', 'coming',
                    'came', 'see', 'saw', 'seen', 'look', 'looked', 'looking', 'looks'}

    high_freq_words = [(word, count) for word, count in word_counts.most_common(200)
                       if count >= 3 and word not in common_words][:100]

    # ä¿å­˜åˆ°æ•°æ®åº“
    db = SessionLocal()
    try:
        # åˆ›å»ºä¹¦ç±è®°å½•
        db_book = Book(
            id=book_id,
            title=title,
            author=author,
            cover=cover_path,  # è®¾ç½®å°é¢
            level=level,
            word_count=total_words,
            description=description,
            epub_path=epub_path
        )
        db.add(db_book)

        # åˆ›å»ºç« èŠ‚è®°å½•
        for chapter_data in chapters_data:
            db_chapter = Chapter(**chapter_data)
            db.add(db_chapter)

        # åˆ›å»ºè¯æ±‡è®°å½•
        for word, freq in high_freq_words:
            db_vocab = BookVocabulary(
                id=str(uuid.uuid4()),
                book_id=book_id,
                word=word,
                frequency=freq
            )
            db.add(db_vocab)

        db.commit()

        # åŒæ—¶å†™å…¥Supabaseï¼ˆå¦‚æœå·²é…ç½®ï¼‰
        if supabase_client.enabled:
            try:
                logger.info("ğŸ“¤ å¼€å§‹åŒæ­¥æ•°æ®åˆ°Supabase...")

                # 1. æ’å…¥ä¹¦ç±æ•°æ®
                book_data_for_supabase = {
                    'id': book_id,
                    'title': title,
                    'author': author,
                    'cover': cover_path,
                    'level': level,
                    'word_count': total_words,
                    'description': description,
                    'epub_path': epub_path,
                }
                supabase_client.insert_book(book_data_for_supabase)

                # 2. æ‰¹é‡æ’å…¥ç« èŠ‚æ•°æ®
                chapters_for_supabase = []
                for chapter_data in chapters_data:
                    chapters_for_supabase.append({
                        'id': chapter_data['id'],
                        'book_id': chapter_data['book_id'],
                        'chapter_number': chapter_data['chapter_number'],
                        'title': chapter_data.get('title'),
                        'content': chapter_data['content'],
                        'word_count': chapter_data['word_count'],
                    })
                supabase_client.bulk_insert_chapters(chapters_for_supabase)

                # 3. æ‰¹é‡æ’å…¥è¯æ±‡æ•°æ®
                vocab_for_supabase = []
                for word, freq in high_freq_words:
                    vocab_for_supabase.append({
                        'id': str(uuid.uuid4()),
                        'book_id': book_id,
                        'word': word,
                        'frequency': freq,
                    })
                supabase_client.bulk_insert_vocabulary(vocab_for_supabase)

                logger.info("âœ… æ•°æ®å·²æˆåŠŸåŒæ­¥åˆ°Supabase")
            except Exception as e:
                logger.warning(f"âš ï¸ SupabaseåŒæ­¥å¤±è´¥ï¼ˆä¸å½±å“æœ¬åœ°SQLiteï¼‰: {e}")

        print(f"âœ… Successfully imported: {title}")
        print(f"   - Author: {author}")
        print(f"   - Chapters: {len(chapters_data)}")
        print(f"   - Total words: {total_words}")
        print(f"   - High-freq vocabulary: {len(high_freq_words)}")
        print(f"   - Images: {len(image_map) // 3}")  # é™¤ä»¥3å› ä¸ºæ¯å¼ å›¾æœ‰å¤šä¸ªæ˜ å°„
        print(f"   - Cover: {cover_path}")
        print(f"   - Book ID: {book_id}")

        return book_id

    except Exception as e:
        db.rollback()
        # æ¸…ç†å·²åˆ›å»ºçš„å›¾ç‰‡
        logger.error(f"å¯¼å…¥ä¹¦ç±å¤±è´¥ï¼Œæ¸…ç†å›¾ç‰‡èµ„æº: {e}")

        # æ¸…ç†OSSå›¾ç‰‡
        if oss_helper.enabled:
            oss_helper.delete_images(book_id)

        # æ¸…ç†æœ¬åœ°å›¾ç‰‡ç›®å½•
        if os.path.exists(images_dir):
            shutil.rmtree(images_dir)

        raise e
    finally:
        db.close()


def main():
    parser = argparse.ArgumentParser(description='Import EPUB book to database')
    parser.add_argument('epub_file', help='Path to EPUB file')
    parser.add_argument('--level', '-l', help='Book level (e.g., å­¦å‰, ä¸€å¹´çº§, åˆä¸€)')

    args = parser.parse_args()

    # ç¡®ä¿æ•°æ®åº“è¡¨å­˜åœ¨
    create_tables()

    # å¯¼å…¥ä¹¦ç±
    import_epub(args.epub_file, args.level)


if __name__ == '__main__':
    main()
