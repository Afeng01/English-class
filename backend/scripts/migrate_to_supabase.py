"""
SQLite åˆ° Supabase æ•°æ®è¿ç§»è„šæœ¬
ç”¨æ³•: python migrate_to_supabase.py [--dry-run]
"""
import argparse
import sys
import os
import logging
from typing import Dict, List, Any

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from app.models.database import SessionLocal, Book, Chapter, BookVocabulary
from app.utils.supabase_client import supabase_client

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class MigrationStats:
    """è¿ç§»ç»Ÿè®¡ä¿¡æ¯"""
    def __init__(self):
        self.books_total = 0
        self.books_migrated = 0
        self.books_failed = 0

        self.chapters_total = 0
        self.chapters_migrated = 0
        self.chapters_failed = 0

        self.vocabulary_total = 0
        self.vocabulary_migrated = 0
        self.vocabulary_failed = 0

    def print_summary(self):
        """æ‰“å°è¿ç§»æ‘˜è¦"""
        logger.info("=" * 60)
        logger.info("ğŸ“Š è¿ç§»ç»Ÿè®¡æ‘˜è¦")
        logger.info("=" * 60)
        logger.info(f"ğŸ“š ä¹¦ç±: {self.books_migrated}/{self.books_total} æˆåŠŸ, {self.books_failed} å¤±è´¥")
        logger.info(f"ğŸ“– ç« èŠ‚: {self.chapters_migrated}/{self.chapters_total} æˆåŠŸ, {self.chapters_failed} å¤±è´¥")
        logger.info(f"ğŸ“ è¯æ±‡: {self.vocabulary_migrated}/{self.vocabulary_total} æˆåŠŸ, {self.vocabulary_failed} å¤±è´¥")
        logger.info("=" * 60)


def migrate_books(db_session, stats: MigrationStats, dry_run: bool = False) -> Dict[str, bool]:
    """
    è¿ç§»ä¹¦ç±æ•°æ®
    è¿”å›: {book_id: success} æ˜ å°„ï¼Œç”¨äºåç»­è¿ç§»ç« èŠ‚å’Œè¯æ±‡
    """
    logger.info("ğŸ“š å¼€å§‹è¿ç§»ä¹¦ç±æ•°æ®...")

    books = db_session.query(Book).all()
    stats.books_total = len(books)

    logger.info(f"æ‰¾åˆ° {stats.books_total} æœ¬ä¹¦")

    book_success_map = {}

    for book in books:
        try:
            book_data = {
                'id': book.id,
                'title': book.title,
                'author': book.author,
                'cover': book.cover,
                'level': book.level,
                'word_count': book.word_count,
                'description': book.description,
                'epub_path': book.epub_path,
                'created_at': book.created_at.isoformat() if book.created_at else None,
            }

            if dry_run:
                logger.info(f"  [DRY RUN] å°†è¿ç§»ä¹¦ç±: {book.title}")
                stats.books_migrated += 1
                book_success_map[book.id] = True
            else:
                success = supabase_client.insert_book(book_data)
                if success:
                    logger.info(f"  âœ… è¿ç§»æˆåŠŸ: {book.title}")
                    stats.books_migrated += 1
                    book_success_map[book.id] = True
                else:
                    logger.error(f"  âŒ è¿ç§»å¤±è´¥: {book.title}")
                    stats.books_failed += 1
                    book_success_map[book.id] = False

        except Exception as e:
            logger.error(f"  âŒ è¿ç§»ä¹¦ç±å‡ºé”™ {book.title}: {e}")
            stats.books_failed += 1
            book_success_map[book.id] = False

    return book_success_map


def migrate_chapters(db_session, book_success_map: Dict[str, bool], stats: MigrationStats, dry_run: bool = False):
    """è¿ç§»ç« èŠ‚æ•°æ®"""
    logger.info("ğŸ“– å¼€å§‹è¿ç§»ç« èŠ‚æ•°æ®...")

    # åªè¿ç§»æˆåŠŸä¹¦ç±çš„ç« èŠ‚
    successful_book_ids = [book_id for book_id, success in book_success_map.items() if success]

    if not successful_book_ids:
        logger.warning("æ²¡æœ‰æˆåŠŸè¿ç§»çš„ä¹¦ç±ï¼Œè·³è¿‡ç« èŠ‚è¿ç§»")
        return

    chapters = db_session.query(Chapter).filter(Chapter.book_id.in_(successful_book_ids)).all()
    stats.chapters_total = len(chapters)

    logger.info(f"æ‰¾åˆ° {stats.chapters_total} ä¸ªç« èŠ‚")

    # æ‰¹é‡è¿ç§»ï¼Œæ¯æ‰¹100ä¸ª
    batch_size = 100
    for i in range(0, len(chapters), batch_size):
        batch = chapters[i:i + batch_size]

        chapters_data = []
        for chapter in batch:
            chapter_data = {
                'id': chapter.id,
                'book_id': chapter.book_id,
                'chapter_number': chapter.chapter_number,
                'title': chapter.title,
                'content': chapter.content,
                'word_count': chapter.word_count,
            }
            chapters_data.append(chapter_data)

        if dry_run:
            logger.info(f"  [DRY RUN] å°†è¿ç§»ç« èŠ‚æ‰¹æ¬¡: {i+1}-{min(i+batch_size, len(chapters))}/{len(chapters)}")
            stats.chapters_migrated += len(batch)
        else:
            try:
                success = supabase_client.bulk_insert_chapters(chapters_data)
                if success:
                    logger.info(f"  âœ… æ‰¹æ¬¡è¿ç§»æˆåŠŸ: {i+1}-{min(i+batch_size, len(chapters))}/{len(chapters)}")
                    stats.chapters_migrated += len(batch)
                else:
                    logger.error(f"  âŒ æ‰¹æ¬¡è¿ç§»å¤±è´¥: {i+1}-{min(i+batch_size, len(chapters))}")
                    stats.chapters_failed += len(batch)
            except Exception as e:
                logger.error(f"  âŒ ç« èŠ‚æ‰¹æ¬¡è¿ç§»å‡ºé”™: {e}")
                stats.chapters_failed += len(batch)


def migrate_vocabulary(db_session, book_success_map: Dict[str, bool], stats: MigrationStats, dry_run: bool = False):
    """è¿ç§»è¯æ±‡æ•°æ®"""
    logger.info("ğŸ“ å¼€å§‹è¿ç§»è¯æ±‡æ•°æ®...")

    # åªè¿ç§»æˆåŠŸä¹¦ç±çš„è¯æ±‡
    successful_book_ids = [book_id for book_id, success in book_success_map.items() if success]

    if not successful_book_ids:
        logger.warning("æ²¡æœ‰æˆåŠŸè¿ç§»çš„ä¹¦ç±ï¼Œè·³è¿‡è¯æ±‡è¿ç§»")
        return

    vocabulary = db_session.query(BookVocabulary).filter(BookVocabulary.book_id.in_(successful_book_ids)).all()
    stats.vocabulary_total = len(vocabulary)

    logger.info(f"æ‰¾åˆ° {stats.vocabulary_total} ä¸ªè¯æ±‡")

    # æ‰¹é‡è¿ç§»ï¼Œæ¯æ‰¹200ä¸ª
    batch_size = 200
    for i in range(0, len(vocabulary), batch_size):
        batch = vocabulary[i:i + batch_size]

        vocab_data = []
        for vocab in batch:
            vocab_item = {
                'id': vocab.id,
                'book_id': vocab.book_id,
                'word': vocab.word,
                'frequency': vocab.frequency,
                'phonetic': vocab.phonetic,
                'definition': vocab.definition,
            }
            vocab_data.append(vocab_item)

        if dry_run:
            logger.info(f"  [DRY RUN] å°†è¿ç§»è¯æ±‡æ‰¹æ¬¡: {i+1}-{min(i+batch_size, len(vocabulary))}/{len(vocabulary)}")
            stats.vocabulary_migrated += len(batch)
        else:
            try:
                success = supabase_client.bulk_insert_vocabulary(vocab_data)
                if success:
                    logger.info(f"  âœ… æ‰¹æ¬¡è¿ç§»æˆåŠŸ: {i+1}-{min(i+batch_size, len(vocabulary))}/{len(vocabulary)}")
                    stats.vocabulary_migrated += len(batch)
                else:
                    logger.error(f"  âŒ æ‰¹æ¬¡è¿ç§»å¤±è´¥: {i+1}-{min(i+batch_size, len(vocabulary))}")
                    stats.vocabulary_failed += len(batch)
            except Exception as e:
                logger.error(f"  âŒ è¯æ±‡æ‰¹æ¬¡è¿ç§»å‡ºé”™: {e}")
                stats.vocabulary_failed += len(batch)


def verify_migration(db_session, stats: MigrationStats) -> bool:
    """éªŒè¯è¿ç§»å®Œæ•´æ€§"""
    logger.info("ğŸ” å¼€å§‹éªŒè¯è¿ç§»...")

    try:
        # éªŒè¯ä¹¦ç±æ•°é‡
        books_in_supabase = supabase_client.list_books()
        if books_in_supabase:
            logger.info(f"  âœ… Supabaseä¸­æœ‰ {len(books_in_supabase)} æœ¬ä¹¦")
            if len(books_in_supabase) == stats.books_migrated:
                logger.info(f"  âœ… ä¹¦ç±æ•°é‡åŒ¹é…")
            else:
                logger.warning(f"  âš ï¸  ä¹¦ç±æ•°é‡ä¸åŒ¹é…: Supabase {len(books_in_supabase)} vs è¿ç§» {stats.books_migrated}")
        else:
            logger.error("  âŒ æ— æ³•è·å–Supabaseä¹¦ç±åˆ—è¡¨")
            return False

        # æŠ½æ ·éªŒè¯ï¼šæ£€æŸ¥ç¬¬ä¸€æœ¬ä¹¦çš„ç« èŠ‚å’Œè¯æ±‡
        if books_in_supabase:
            first_book = books_in_supabase[0]
            book_id = first_book.get('id')

            chapters = supabase_client.get_chapters(book_id)
            vocabulary = supabase_client.get_book_vocabulary(book_id)

            logger.info(f"  ğŸ“Š æŠ½æ ·éªŒè¯ä¹¦ç± {first_book.get('title')}:")
            logger.info(f"    - ç« èŠ‚æ•°: {len(chapters) if chapters else 0}")
            logger.info(f"    - è¯æ±‡æ•°: {len(vocabulary) if vocabulary else 0}")

        logger.info("âœ… éªŒè¯å®Œæˆ")
        return True

    except Exception as e:
        logger.error(f"âŒ éªŒè¯å¤±è´¥: {e}")
        return False


def main():
    parser = argparse.ArgumentParser(description='Migrate SQLite data to Supabase')
    parser.add_argument(
        '--dry-run',
        action='store_true',
        help='æ¨¡æ‹Ÿè¿è¡Œï¼Œä¸å®é™…è¿ç§»æ•°æ®'
    )
    parser.add_argument(
        '--skip-verification',
        action='store_true',
        help='è·³è¿‡è¿ç§»åçš„éªŒè¯æ­¥éª¤'
    )

    args = parser.parse_args()

    # æ£€æŸ¥Supabaseæ˜¯å¦å·²é…ç½®
    if not supabase_client.enabled:
        logger.error("âŒ Supabaseæœªé…ç½®æˆ–é…ç½®æ— æ•ˆ")
        logger.error("è¯·æ£€æŸ¥.envæ–‡ä»¶ä¸­çš„SUPABASE_URLå’ŒSUPABASE_SERVICE_KEY")
        sys.exit(1)

    if args.dry_run:
        logger.info("ğŸ”„ è¿è¡Œæ¨¡å¼: DRY RUN (æ¨¡æ‹Ÿè¿è¡Œ)")
    else:
        logger.info("ğŸ”„ è¿è¡Œæ¨¡å¼: å®é™…è¿ç§»")
        logger.warning("âš ï¸  æ­¤æ“ä½œå°†å‘Supabaseå†™å…¥æ•°æ®ï¼Œè¯·ç¡®è®¤å·²åšå¥½å¤‡ä»½ï¼")

        # ç­‰å¾…ç”¨æˆ·ç¡®è®¤
        response = input("æ˜¯å¦ç»§ç»­? (yes/no): ").strip().lower()
        if response not in ['yes', 'y']:
            logger.info("âŒ ç”¨æˆ·å–æ¶ˆè¿ç§»")
            sys.exit(0)

    logger.info("=" * 60)
    logger.info("ğŸš€ å¼€å§‹æ•°æ®è¿ç§»")
    logger.info("=" * 60)

    # åˆ›å»ºæ•°æ®åº“ä¼šè¯
    db = SessionLocal()
    stats = MigrationStats()

    try:
        # 1. è¿ç§»ä¹¦ç±
        book_success_map = migrate_books(db, stats, dry_run=args.dry_run)

        # 2. è¿ç§»ç« èŠ‚
        migrate_chapters(db, book_success_map, stats, dry_run=args.dry_run)

        # 3. è¿ç§»è¯æ±‡
        migrate_vocabulary(db, book_success_map, stats, dry_run=args.dry_run)

        # 4. æ‰“å°ç»Ÿè®¡
        stats.print_summary()

        # 5. éªŒè¯è¿ç§»ï¼ˆédry-runæ¨¡å¼ï¼‰
        if not args.dry_run and not args.skip_verification:
            verify_migration(db, stats)

        if args.dry_run:
            logger.info("âœ… æ¨¡æ‹Ÿè¿è¡Œå®Œæˆï¼Œæœªå®é™…è¿ç§»æ•°æ®")
        else:
            logger.info("âœ… è¿ç§»å®Œæˆï¼")
            logger.info("ğŸ’¡ æç¤º: ç°åœ¨å¯ä»¥ä½¿ç”¨Supabaseä½œä¸ºæ•°æ®æº")

    except Exception as e:
        logger.error(f"âŒ è¿ç§»è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        sys.exit(1)

    finally:
        db.close()


if __name__ == '__main__':
    main()
