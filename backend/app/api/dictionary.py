from fastapi import APIRouter, HTTPException
import asyncio
import httpx
import os
import hashlib
import time
import uuid
import json
import re
from datetime import datetime, timedelta
from typing import Optional, Dict
from nltk.stem import WordNetLemmatizer
from nltk.corpus import wordnet
from functools import lru_cache

from app.schemas.schemas import DictionaryResponse

router = APIRouter()

# ==================== æœ‰é“è¯å…¸APIé…ç½® ====================
# ç”³è¯·åœ°å€ï¼šhttps://ai.youdao.com/
YOUDAO_APP_KEY = os.getenv("YOUDAO_APP_KEY", "")
YOUDAO_APP_SECRET = os.getenv("YOUDAO_APP_SECRET", "")
YOUDAO_DICT_API = "https://openapi.youdao.com/api"

# ==================== ç¼“å­˜é…ç½® ====================
# è¯å…¸æŸ¥è¯¢ç»“æœç¼“å­˜ï¼ˆå†…å­˜ç¼“å­˜ï¼ŒæœåŠ¡é‡å¯åæ¸…ç©ºï¼‰
_dictionary_cache: Dict[str, tuple[dict, float]] = {}
CACHE_EXPIRE_HOURS = 24  # ç¼“å­˜24å°æ—¶

# åˆå§‹åŒ–è¯å½¢è¿˜åŸå™¨
lemmatizer = WordNetLemmatizer()
PUNCTUATION_MARKS = set(",.;!?ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼šã€â€œâ€\"'()")
EXPLAIN_SPLIT_PATTERN = re.compile(r'[ï¼›;ï¼Œã€]+')


# ==================== ç¼“å­˜è¾…åŠ©å‡½æ•° ====================
def get_from_cache(word: str) -> Optional[dict]:
    """ä»ç¼“å­˜ä¸­è·å–è¯å…¸ç»“æœ"""
    word_lower = word.lower()
    if word_lower in _dictionary_cache:
        result, expire_time = _dictionary_cache[word_lower]
        if datetime.now().timestamp() < expire_time:
            return result
        else:
            del _dictionary_cache[word_lower]
    return None


def is_phrase_or_sentence(text: str) -> bool:
    """ç®€å•åˆ¤æ–­æŸ¥è¯¢æ˜¯å¦ä¸ºçŸ­è¯­/å¥å­ï¼ˆåŒ…å«ç©ºæ ¼æˆ–æ ‡ç‚¹ï¼‰"""
    stripped = text.strip()
    if not stripped:
        return False
    if any(ch.isspace() for ch in stripped):
        return True
    return any(ch in PUNCTUATION_MARKS for ch in stripped)


def save_to_cache(word: str, result: dict):
    """ä¿å­˜è¯å…¸ç»“æœåˆ°ç¼“å­˜"""
    word_lower = word.lower()
    expire_time = (datetime.now() + timedelta(hours=CACHE_EXPIRE_HOURS)).timestamp()
    _dictionary_cache[word_lower] = (result, expire_time)


# ==================== è¯å½¢è¿˜åŸ ====================
# ä¸è§„åˆ™åŠ¨è¯æ˜ å°„ï¼ˆNLTKå¤„ç†ä¸å¥½çš„æƒ…å†µï¼‰
IRREGULAR_VERBS = {
    "was": "be", "were": "be", "been": "be", "am": "be", "is": "be", "are": "be",
    "had": "have", "has": "have",
    "did": "do", "does": "do", "done": "do",
    "went": "go", "gone": "go", "goes": "go",
    "said": "say", "says": "say",
    "made": "make", "makes": "make",
    "knew": "know", "known": "know", "knows": "know",
    "thought": "think", "thinks": "think",
    "took": "take", "taken": "take", "takes": "take",
    "saw": "see", "seen": "see", "sees": "see",
    "came": "come", "comes": "come",
    "gave": "give", "given": "give", "gives": "give",
    "got": "get", "gotten": "get", "gets": "get",
    "found": "find", "finds": "find",
    "told": "tell", "tells": "tell",
    "felt": "feel", "feels": "feel",
    "became": "become", "becomes": "become",
    "left": "leave", "leaves": "leave",
    "brought": "bring", "brings": "bring",
    "began": "begin", "begun": "begin", "begins": "begin",
    "kept": "keep", "keeps": "keep",
    "held": "hold", "holds": "hold",
    "wrote": "write", "written": "write", "writes": "write",
    "stood": "stand", "stands": "stand",
    "heard": "hear", "hears": "hear",
    "let": "let", "lets": "let",
    "meant": "mean", "means": "mean",
    "set": "set", "sets": "set",
    "met": "meet", "meets": "meet",
    "ran": "run", "runs": "run",
    "paid": "pay", "pays": "pay",
    "sat": "sit", "sits": "sit",
    "spoke": "speak", "spoken": "speak", "speaks": "speak",
    "lay": "lie", "lain": "lie", "lies": "lie",
    "led": "lead", "leads": "lead",
    "read": "read", "reads": "read",
    "grew": "grow", "grown": "grow", "grows": "grow",
    "lost": "lose", "loses": "lose",
    "fell": "fall", "fallen": "fall", "falls": "fall",
    "sent": "send", "sends": "send",
    "built": "build", "builds": "build",
    "spent": "spend", "spends": "spend",
    "won": "win", "wins": "win",
    "caught": "catch", "catches": "catch",
    "taught": "teach", "teaches": "teach",
    "bought": "buy", "buys": "buy",
    "wore": "wear", "worn": "wear", "wears": "wear",
    "chose": "choose", "chosen": "choose", "chooses": "choose",
    "broke": "break", "broken": "break", "breaks": "break",
    "drove": "drive", "driven": "drive", "drives": "drive",
    "ate": "eat", "eaten": "eat", "eats": "eat",
    "drew": "draw", "drawn": "draw", "draws": "draw",
    "flew": "fly", "flown": "fly", "flies": "fly",
    "threw": "throw", "thrown": "throw", "throws": "throw",
    "children": "child",
    "men": "man",
    "women": "woman",
    "feet": "foot",
    "teeth": "tooth",
    "mice": "mouse",
    "geese": "goose",
    "people": "person",
}


@lru_cache(maxsize=1000)
def lemmatize_word(word: str) -> str:
    """è¯å½¢è¿˜åŸï¼šå°†è¯å½¢å˜åŒ–è¿˜åŸä¸ºåŸå½¢

    ä¾‹å¦‚ï¼šrunning â†’ run, went â†’ go, children â†’ child
    """
    word_lower = word.lower()

    # 1. é¦–å…ˆæ£€æŸ¥ä¸è§„åˆ™åŠ¨è¯æ˜ å°„
    if word_lower in IRREGULAR_VERBS:
        return IRREGULAR_VERBS[word_lower]

    # 2. ä½¿ç”¨WordNetè¿›è¡Œè¿˜åŸ
    pos_list = [wordnet.VERB, wordnet.NOUN, wordnet.ADJ, wordnet.ADV]
    results = set()

    for pos in pos_list:
        lemma = lemmatizer.lemmatize(word_lower, pos=pos)
        if lemma != word_lower:
            results.add(lemma)

    # å¦‚æœæœ‰è¿˜åŸç»“æœï¼Œä¼˜å…ˆè¿”å›æœ€çŸ­çš„ç»“æœ
    if results:
        return min(results, key=len)

    # æ— æ³•è¿˜åŸåˆ™è¿”å›åŸè¯å°å†™
    return word_lower


# ==================== æœ‰é“è¯å…¸API ====================
def truncate(q: str) -> str:
    """æˆªæ–­æ–‡æœ¬ï¼ˆæœ‰é“APIç­¾åè¦æ±‚ï¼‰

    å¦‚æœæ–‡æœ¬é•¿åº¦ â‰¤ 20ï¼Œç›´æ¥è¿”å›
    å¦‚æœæ–‡æœ¬é•¿åº¦ > 20ï¼Œè¿”å›ï¼šå‰10ä¸ªå­—ç¬¦ + é•¿åº¦ + å10ä¸ªå­—ç¬¦
    """
    if q is None:
        return None
    size = len(q)
    return q if size <= 20 else q[0:10] + str(size) + q[size - 10:size]


async def query_free_dictionary(client: httpx.AsyncClient, word: str) -> Optional[dict]:
    """æŸ¥è¯¢ Free Dictionary APIï¼ˆè‹±æ–‡è¯å…¸ï¼Œå…è´¹ï¼‰"""
    start_time = time.time()
    try:
        url = f"https://api.dictionaryapi.dev/api/v2/entries/en/{word.lower()}"
        response = await client.get(url, timeout=2.0)

        if response.status_code != 200:
            elapsed = (time.time() - start_time) * 1000
            print(f"âŒ Free Dictionary HTTPé”™è¯¯ {response.status_code}: {word} ({elapsed:.0f}ms)")
            return None

        data = response.json()
        if not data or not isinstance(data, list) or len(data) == 0:
            elapsed = (time.time() - start_time) * 1000
            print(f"â„¹ï¸ Free Dictionaryæ— ç»“æœ: {word} ({elapsed:.0f}ms)")
            return None

        entry = data[0]

        phonetic = entry.get('phonetic', '')
        if not phonetic and 'phonetics' in entry:
            for p in entry.get('phonetics', []):
                if p.get('text'):
                    phonetic = p['text']
                    break

        meanings = []
        for meaning in entry.get('meanings', []):
            part_of_speech = meaning.get('partOfSpeech', '')
            definitions = []

            for definition in meaning.get('definitions', []):
                definitions.append({
                    "definition": definition.get('definition', ''),
                    "example": definition.get('example', '')
                })

            if definitions:
                meanings.append({
                    "partOfSpeech": part_of_speech,
                    "definitions": definitions,
                    "lang": "en"
                })

        if not meanings:
            elapsed = (time.time() - start_time) * 1000
            print(f"â„¹ï¸ Free Dictionaryæ— é‡Šä¹‰: {word} ({elapsed:.0f}ms)")
            return None

        elapsed = (time.time() - start_time) * 1000
        print(f"âœ… Free Dictionaryè¿”å› {len(meanings)} ä¸ªè¯æ€§é‡Šä¹‰: {word} ({elapsed:.0f}ms)")
        return {
            "word": entry.get('word', word),
            "phonetic": phonetic,
            "meanings": meanings,
        }

    except httpx.TimeoutException:
        elapsed = (time.time() - start_time) * 1000
        print(f"â±ï¸ Free Dictionaryè¶…æ—¶: {word} ({elapsed:.0f}ms)")
        return None
    except Exception as e:
        elapsed = (time.time() - start_time) * 1000
        print(f"Free Dictionaryå¼‚å¸¸: {e} ({elapsed:.0f}ms)")
        return None


async def query_youdao_translate(client: httpx.AsyncClient, word: str) -> Optional[dict]:
    """æŸ¥è¯¢æœ‰é“ç¿»è¯‘APIï¼ˆç”¨äºçŸ­è¯­å’Œå¥å­çš„ä¸­æ–‡ç¿»è¯‘ï¼‰

    è¿”å›æ ¼å¼ï¼š
    {
        "word": "hello world",
        "phonetic": "",
        "meanings": [
            {
                "partOfSpeech": "",
                "definitions": [{"definition": "ä½ å¥½ä¸–ç•Œ", "example": ""}]
            }
        ]
    }
    """
    # æ£€æŸ¥APIé…ç½®ï¼ˆåŒ…æ‹¬æ£€æµ‹å ä½ç¬¦å€¼ï¼‰
    if not YOUDAO_APP_KEY or not YOUDAO_APP_SECRET or \
       YOUDAO_APP_KEY == "your_app_key_here" or \
       YOUDAO_APP_SECRET == "your_app_secret_here":
        print(f"âš ï¸  æœ‰é“APIæœªé…ç½®ï¼Œè·³è¿‡ç¿»è¯‘")
        return None

    start_time = time.time()
    try:
        # ç”Ÿæˆè¯·æ±‚å‚æ•°
        salt = str(uuid.uuid4())
        curtime = str(int(time.time()))
        q = word.lower()

        # è®¡ç®—ç­¾åï¼šsign = sha256(appKey + truncate(q) + salt + curtime + appSecret)
        # æ³¨æ„ï¼šå¿…é¡»ä½¿ç”¨ truncate(q)ï¼Œä¸èƒ½ç›´æ¥ç”¨ q
        sign_str = YOUDAO_APP_KEY + truncate(q) + salt + curtime + YOUDAO_APP_SECRET
        sign = hashlib.sha256(sign_str.encode('utf-8')).hexdigest()

        params = {
            'q': q,
            'from': 'en',
            'to': 'zh-CHS',
            'appKey': YOUDAO_APP_KEY,
            'salt': salt,
            'sign': sign,
            'signType': 'v3',
            'curtime': curtime,
        }

        response = await client.get(YOUDAO_DICT_API, params=params, timeout=3.0)

        if response.status_code != 200:
            elapsed = (time.time() - start_time) * 1000
            print(f"âŒ æœ‰é“API HTTPé”™è¯¯: {response.status_code} ({elapsed:.0f}ms)")
            return None

        data = response.json()
        try:
            print("=" * 50)
            print(f"ğŸ“¥ æœ‰é“APIå®Œæ•´å“åº”({word}):")
            print(json.dumps(data, ensure_ascii=False, indent=2))
            print("=" * 50)
        except Exception as log_error:
            print(f"âš ï¸ æœ‰é“APIå“åº”æ—¥å¿—å†™å…¥å¤±è´¥: {log_error}")

        # æ£€æŸ¥é”™è¯¯ç 
        error_code = data.get('errorCode')
        if error_code != '0':
            elapsed = (time.time() - start_time) * 1000
            print(f"âŒ æœ‰é“APIé”™è¯¯ç : {error_code} ({elapsed:.0f}ms)")
            # å¸¸è§é”™è¯¯ç è¯´æ˜
            error_messages = {
                '101': 'ç¼ºå°‘å¿…å¡«çš„å‚æ•°',
                '102': 'ä¸æ”¯æŒçš„è¯­è¨€ç±»å‹',
                '103': 'ç¿»è¯‘æ–‡æœ¬è¿‡é•¿',
                '108': 'åº”ç”¨IDæ— æ•ˆ',
                '110': 'æ— ç›¸å…³æœåŠ¡çš„æœ‰æ•ˆå®ä¾‹',
                '111': 'å¼€å‘è€…è´¦å·æ— æ•ˆ',
                '113': 'qä¸èƒ½ä¸ºç©º',
                '202': 'ç­¾åæ£€éªŒå¤±è´¥',
                '401': 'è´¦æˆ·å·²ç»æ¬ è´¹',
                '411': 'è®¿é—®é¢‘ç‡å—é™'
            }
            if error_code in error_messages:
                print(f"   {error_messages[error_code]}")
            return None

        # è§£ææœ‰é“è¯å…¸å“åº”
        basic = data.get('basic', {})
        translation = data.get('translation', [])
        print(f"ğŸ“‘ basicå­—æ®µ: {json.dumps(basic, ensure_ascii=False) if basic else '{}'}")
        print(f"ğŸŒ webå­—æ®µæ•°é‡: {len(data.get('web', []) or [])}")
        print(f"ğŸ” translationå­—æ®µ: {json.dumps(translation, ensure_ascii=False)}")

        if not basic and not translation:
            return None

        # æ„é€ meanings
        meanings = []

        # ä»basicæå–é‡Šä¹‰ï¼ˆä¸é™åˆ¶æ•°é‡ï¼Œæ˜¾ç¤ºå…¨éƒ¨ï¼‰
        explains = basic.get('explains', [])
        if explains:
            for explain in explains:  # æ˜¾ç¤ºå…¨éƒ¨é‡Šä¹‰
                text = explain.strip()
                part_of_speech = ""
                content = text
                if '.' in text:
                    prefix, rest = text.split('.', 1)
                    if len(prefix.strip()) <= 6:
                        part_of_speech = prefix.strip()
                        content = rest.strip()
                fragments = [frag.strip() for frag in EXPLAIN_SPLIT_PATTERN.split(content) if frag.strip()]
                if not fragments:
                    fragments = [content]
                meanings.append({
                    "partOfSpeech": part_of_speech,
                    "definitions": [{
                        "definition": fragment,
                        "example": ""
                    } for fragment in fragments],
                    "lang": "zh"  # æ ‡è®°ä¸ºä¸­æ–‡ç¿»è¯‘
                })

        # è¡¥å……æœºå™¨ç¿»è¯‘ç»“æœ
        if translation:
            for trans in translation:
                if not trans:
                    continue
                meanings.append({
                    "partOfSpeech": "ç¿»è¯‘",
                    "definitions": [{
                        "definition": trans,
                        "example": ""
                    }],
                    "lang": "zh"
                })

        # è§£æ web å­—æ®µçš„ç½‘ç»œé‡Šä¹‰ï¼ˆé€šå¸¸åŒ…å«æ›´å£è¯­åŒ–çš„ç¿»è¯‘ï¼‰
        web_entries = data.get('web', [])
        for entry in web_entries:
            values = entry.get('value', [])
            if not values:
                continue

            definitions = []
            for value in values:
                if not value:
                    continue
                definitions.append({
                    "definition": value,
                    "example": entry.get('key', "")
                })

            if definitions:
                meanings.append({
                    "partOfSpeech": "ç½‘ç»œé‡Šä¹‰",
                    "definitions": definitions,
                    "lang": "zh"
                })

        # è§£æè¯å½¢å˜åŒ–
        wfs = basic.get('wfs', [])
        wf_definitions = []
        for wf_entry in wfs:
            wf = wf_entry.get('wf', {})
            value = wf.get('value')
            if not value:
                continue
            name = wf.get('name')
            label = f"{name or 'è¯å½¢'}: {value}"
            wf_definitions.append({
                "definition": label,
                "example": ""
            })
        if wf_definitions:
            meanings.append({
                "partOfSpeech": "è¯å½¢å˜åŒ–",
                "definitions": wf_definitions,
                "lang": "zh"
            })

        # è§£æä¾‹å¥ï¼ˆè‹¥æœ‰ï¼‰
        def add_sentence_meanings(entries, label: str):
            if not entries:
                return
            sentence_definitions = []
            for sentence in entries:
                if not isinstance(sentence, dict):
                    continue
                cn = sentence.get('sCn') or sentence.get('cn') or sentence.get('tran') or sentence.get('translation') or sentence.get('target')
                en = sentence.get('sContent') or sentence.get('content') or sentence.get('source') or sentence.get('sentence')
                text = cn or en
                if not text:
                    continue
                sentence_definitions.append({
                    "definition": text,
                    "example": en or ""
                })
            if sentence_definitions:
                meanings.append({
                    "partOfSpeech": label,
                    "definitions": sentence_definitions,
                    "lang": "zh"
                })

        sentence_entries = data.get('sentence') or []
        sentence_entries_alt = data.get('sentences') or []
        example_entries = data.get('examples') or []
        add_sentence_meanings(sentence_entries, "ä¾‹å¥")
        add_sentence_meanings(sentence_entries_alt, "ä¾‹å¥")
        add_sentence_meanings(example_entries, "ä¾‹å¥")

        if not meanings:
            return None

        result = {
            "word": word,
            "phonetic": basic.get('phonetic', '') or basic.get('us-phonetic', '') or basic.get('uk-phonetic', ''),
            "meanings": meanings,
        }

        try:
            print(
                f"ğŸ“Š æœ‰é“APIç»Ÿè®¡({word}): explains={len(explains)} "
                f"translation={len(translation)} web={len(web_entries)} "
                f"wfs={len(wfs)} sentence={len(sentence_entries) + len(sentence_entries_alt) + len(example_entries)} meanings={len(meanings)}"
            )
            print(f"ğŸ“š æœ‰é“è§£æé‡Šä¹‰({word}): {json.dumps(meanings, ensure_ascii=False)}")
        except Exception as log_error:
            print(f"âš ï¸ æœ‰é“è§£ææ—¥å¿—å†™å…¥å¤±è´¥: {log_error}")
        elapsed = (time.time() - start_time) * 1000
        print(f"âœ… æœ‰é“APIè¿”å› {len(meanings)} æ¡é‡Šä¹‰: {word} ({elapsed:.0f}ms)")

        return result

    except httpx.TimeoutException:
        elapsed = (time.time() - start_time) * 1000
        print(f"æœ‰é“APIè¶…æ—¶: {word} ({elapsed:.0f}ms)")
        return None
    except Exception as e:
        elapsed = (time.time() - start_time) * 1000
        print(f"æœ‰é“APIå¼‚å¸¸: {e} ({elapsed:.0f}ms)")
        return None


def parse_dictionary_entry(entry: dict, original_word: str, lemma: str = None) -> DictionaryResponse:
    """è§£æè¯å…¸APIè¿”å›çš„æ•°æ®"""
    return DictionaryResponse(
        word=entry.get("word", original_word),
        phonetic=entry.get("phonetic", ""),
        meanings=entry.get("meanings", []),
        audio=entry.get("audio"),
        searched_word=original_word if lemma and lemma != original_word.lower() else None,
        lemma=lemma if lemma and lemma != original_word.lower() else None
    )


@router.get("/{word}", response_model=DictionaryResponse)
async def lookup_word(word: str):
    """æŸ¥è¯¢å•è¯é‡Šä¹‰ï¼ˆåŒæ—¶æŸ¥è¯¢ä¸­è‹±æ–‡ï¼Œæ”¯æŒè¯å½¢è¿˜åŸï¼‰

    æŸ¥è¯¢ç­–ç•¥ï¼š
    1. æŸ¥è¯¢è‹±æ–‡é‡Šä¹‰ï¼ˆFree Dictionary APIï¼‰
    2. å¦‚æœè‹±æ–‡æŸ¥ä¸åˆ°ï¼Œå°è¯•è¯å½¢è¿˜åŸ
    3. åŒæ—¶æŸ¥è¯¢ä¸­æ–‡ç¿»è¯‘ï¼ˆæœ‰é“APIï¼‰
    4. åˆå¹¶æ‰€æœ‰ç»“æœï¼Œå‰ç«¯æ§åˆ¶æ˜¾ç¤ºå“ªç§è¯­è¨€
    """
    import time
    start_time = time.time()
    query_type = "phrase" if is_phrase_or_sentence(word) else "word"
    print(f"ğŸ” æŸ¥è¯¢è¯·æ±‚: {word} (ç±»å‹: {query_type})")

    # 0. å…ˆæ£€æŸ¥ç¼“å­˜
    cache_check_start = time.time()
    cached_result = get_from_cache(word)
    cache_elapsed = (time.time() - cache_check_start) * 1000
    print(f"â±ï¸ ç¼“å­˜æ£€æŸ¥è€—æ—¶: {cache_elapsed:.0f}ms (å‘½ä¸­: {'æ˜¯' if cached_result else 'å¦'})")
    if cached_result:
        total_elapsed = (time.time() - start_time) * 1000
        print(f"âœ… ç¼“å­˜å‘½ä¸­: {word} (æ€»è€—æ—¶ {total_elapsed:.0f}ms)")
        return DictionaryResponse(**cached_result)

    async with httpx.AsyncClient() as client:
        try:
            lemma = None
            english_entry = None
            chinese_entry = None

            if query_type == "phrase":
                phrase_start = time.time()
                chinese_entry = await query_youdao_translate(client, word)
                phrase_elapsed = (time.time() - phrase_start) * 1000
                print(f"âš¡ æŸ¥è¯¢è·¯çº¿: çŸ­è¯­/å¥å­ â†’ æœ‰é“ ({phrase_elapsed:.0f}ms)")
                if not chinese_entry:
                    print("âš ï¸ çŸ­è¯­ç¿»è¯‘ä¸ºç©ºï¼Œå°è¯•è‹±æ–‡è¯å…¸å›é€€")
                    english_entry = await query_free_dictionary(client, word)
            else:
                api_start = time.time()
                english_result, chinese_result = await asyncio.gather(
                    query_free_dictionary(client, word),
                    query_youdao_translate(client, word),
                    return_exceptions=True
                )
                elapsed = (time.time() - api_start) * 1000
                print(f"âš¡ æŸ¥è¯¢è·¯çº¿: å•è¯ â†’ å¹¶å‘(è‹±æ–‡+ä¸­æ–‡) ({elapsed:.0f}ms)")

                english_entry = None if isinstance(english_result, Exception) else english_result
                chinese_entry = None if isinstance(chinese_result, Exception) else chinese_result

                if isinstance(english_result, Exception):
                    print(f"âŒ è‹±æ–‡é‡Šä¹‰æŸ¥è¯¢å¼‚å¸¸: {english_result}")
                if isinstance(chinese_result, Exception):
                    print(f"âŒ ä¸­æ–‡ç¿»è¯‘æŸ¥è¯¢å¼‚å¸¸: {chinese_result}")

                # è¯å½¢è¿˜åŸé‡è¯•ä»…é’ˆå¯¹è‹±æ–‡é‡Šä¹‰
                if not english_entry:
                    lemma_candidate = lemmatize_word(word)
                    if lemma_candidate != word.lower():
                        lemma = lemma_candidate
                        print(f"ğŸ”„ è¯å½¢è¿˜åŸ: {word} â†’ {lemma}")
                        retry_start = time.time()
                        retry_result = await query_free_dictionary(client, lemma)
                        retry_elapsed = (time.time() - retry_start) * 1000
                        print(f"â†©ï¸  è¯å½¢è¿˜åŸè‹±æ–‡æŸ¥è¯¢è€—æ—¶: {retry_elapsed:.0f}ms")
                        english_entry = retry_result

            # 4. åˆå¹¶ç»“æœ
            if not english_entry and not chinese_entry:
                # ä¸¤è€…éƒ½å¤±è´¥
                elapsed = (time.time() - start_time) * 1000
                print(f"âŒ æœªæ‰¾åˆ°: {word} ({elapsed:.0f}ms)")
                raise HTTPException(
                    status_code=404,
                    detail={
                        "error": "Word not found",
                        "message": f"æœªæ‰¾åˆ° '{word}' çš„é‡Šä¹‰",
                        "word": word,
                        "hint": "è‹±æ–‡è¯å…¸å’Œä¸­æ–‡ç¿»è¯‘éƒ½æœªæ‰¾åˆ°ç»“æœ"
                    }
                )

            # åˆå¹¶è‹±æ–‡å’Œä¸­æ–‡çš„ meanings
            combined_meanings = []
            phonetic = ""

            if english_entry:
                combined_meanings.extend(english_entry.get("meanings", []))
                phonetic = english_entry.get("phonetic", "")
                print(f"âœ… è‹±æ–‡é‡Šä¹‰: {len(english_entry.get('meanings', []))} æ¡")

            if chinese_entry:
                combined_meanings.extend(chinese_entry.get("meanings", []))
                # å¦‚æœè‹±æ–‡æ²¡æœ‰éŸ³æ ‡ï¼Œä½¿ç”¨ä¸­æ–‡çš„
                if not phonetic:
                    phonetic = chinese_entry.get("phonetic", "")
                print(f"âœ… ä¸­æ–‡ç¿»è¯‘: {len(chinese_entry.get('meanings', []))} æ¡")

            # æ„é€ å“åº”
            result = DictionaryResponse(
                word=word,
                phonetic=phonetic,
                meanings=combined_meanings,
                searched_word=word if lemma and lemma != word.lower() else None,
                lemma=lemma if lemma and lemma != word.lower() else None
            )

            # ç¼“å­˜ç»“æœ
            save_to_cache(word, result.model_dump())

            elapsed = (time.time() - start_time) * 1000
            print(f"âœ… æŸ¥è¯¢æˆåŠŸ: {word} (æ€»è€—æ—¶ {elapsed:.0f}ms)")
            return result

        except HTTPException:
            total_elapsed = (time.time() - start_time) * 1000
            print(f"âŒ æŸ¥è¯¢å¤±è´¥(HTTP): {word} ({total_elapsed:.0f}ms)")
            raise
        except Exception as e:
            elapsed = (time.time() - start_time) * 1000
            print(f"âŒ æŸ¥è¯¢é”™è¯¯: {e} ({elapsed:.0f}ms)")
            raise HTTPException(
                status_code=500,
                detail={
                    "error": "Internal server error",
                    "message": f"æŸ¥è¯¢å¤±è´¥: {str(e)}"
                }
            )
