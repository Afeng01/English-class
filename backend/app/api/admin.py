import json
import logging
import os
from datetime import datetime
from typing import List, Optional, Tuple

from fastapi import APIRouter, Depends, HTTPException
from sqlalchemy.orm import Session, selectinload

from app.api.books import delete_book
from app.middleware.admin_check import require_admin_mode
from app.models.database import Book, get_db
from app.schemas.schemas import (
    AdminDeleteFailure,
    AdminDeleteRequest,
    AdminDeleteResponse,
    BackupFailure,
    BackupItem,
    BackupRequest,
    BackupResponse,
    BookResponse,
)

logger = logging.getLogger(__name__)
router = APIRouter(
    prefix="/admin",
    tags=["admin"],
    dependencies=[Depends(require_admin_mode)]
)

_BACKEND_ROOT = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
_BACKUP_DIR = os.path.join(_BACKEND_ROOT, "data", "backups")


def _sanitize_book_id(book_id: str) -> str:
    """å°†ä¹¦ç±IDè½¬æ¢ä¸ºå®‰å…¨çš„æ–‡ä»¶åç‰‡æ®µ"""
    return book_id.replace("/", "_").replace("\\", "_")


def _ensure_backup_dir() -> None:
    """ç¡®ä¿å¤‡ä»½ç›®å½•å­˜åœ¨"""
    os.makedirs(_BACKUP_DIR, exist_ok=True)


def _relative_backup_path(absolute_path: str) -> str:
    """ç”Ÿæˆç›¸å¯¹äºä»“åº“ backend ç›®å½•çš„è·¯å¾„ï¼Œä¿æŒæ—¥å¿—ä¸å“åº”ä¸€è‡´"""
    relative = os.path.relpath(absolute_path, _BACKEND_ROOT)
    normalized = relative.replace(os.sep, "/")
    return f"backend/{normalized}" if not normalized.startswith("backend/") else normalized


def _load_book_with_relations(book_id: str, db: Session) -> Optional[Book]:
    """åŠ è½½ä¹¦ç±åŠå…¶å…³è”è®°å½•ï¼Œä¾›å¤‡ä»½/åˆ é™¤ä½¿ç”¨"""
    return (
        db.query(Book)
        .options(
            selectinload(Book.chapters),
            selectinload(Book.vocabulary)
        )
        .filter(Book.id == book_id)
        .first()
    )


def _serialize_book_bundle(book: Book) -> dict:
    """å°†ä¹¦ç±ã€ç« èŠ‚ã€è¯æ±‡åºåˆ—åŒ–ä¸ºJSONå‹å¥½æ ¼å¼"""
    return {
        "book": {
            "id": book.id,
            "title": book.title,
            "author": book.author,
            "cover": book.cover,
            "level": book.level,
            "lexile": book.lexile,
            "series": book.series,
            "category": book.category,
            "word_count": book.word_count,
            "description": book.description,
            "epub_path": book.epub_path,
            "created_at": book.created_at.isoformat() if book.created_at else None,
        },
        "chapters": [
            {
                "id": chapter.id,
                "book_id": chapter.book_id,
                "chapter_number": chapter.chapter_number,
                "title": chapter.title,
                "content": chapter.content,
                "word_count": chapter.word_count,
            }
            for chapter in sorted(book.chapters, key=lambda c: c.chapter_number)
        ],
        "vocabulary": [
            {
                "id": vocab.id,
                "book_id": vocab.book_id,
                "word": vocab.word,
                "frequency": vocab.frequency,
                "phonetic": vocab.phonetic,
                "definition": vocab.definition,
            }
            for vocab in sorted(book.vocabulary, key=lambda v: v.frequency, reverse=True)
        ],
    }


def _backup_single_book(book_id: str, db: Session) -> Tuple[Optional[BackupItem], Optional[BackupFailure]]:
    """å¤‡ä»½å•æœ¬ä¹¦ç±ï¼Œè¿”å›æˆåŠŸæˆ–å¤±è´¥ç»“æœ"""
    book = _load_book_with_relations(book_id, db)
    if not book:
        return None, BackupFailure(book_id=book_id, reason="ä¹¦ç±ä¸å­˜åœ¨")

    try:
        _ensure_backup_dir()
        timestamp = datetime.utcnow().strftime("%Y%m%d-%H%M%S")
        safe_id = _sanitize_book_id(book_id)
        file_name = f"book_{safe_id}_{timestamp}.json"
        absolute_path = os.path.join(_BACKUP_DIR, file_name)
        payload = _serialize_book_bundle(book)
        with open(absolute_path, "w", encoding="utf-8") as fp:
            json.dump(payload, fp, ensure_ascii=False, indent=2)

        backup_size = os.path.getsize(absolute_path)
        backup_item = BackupItem(
            book_id=book_id,
            backup_path=_relative_backup_path(absolute_path),
            backup_size=backup_size
        )
        logger.info("âœ… ä¹¦ç±å¤‡ä»½å®Œæˆ book_id=%s path=%s size=%sB", book_id, backup_item.backup_path, backup_size)
        return backup_item, None
    except Exception as exc:
        logger.exception("âŒ ä¹¦ç±å¤‡ä»½å¤±è´¥ book_id=%s error=%s", book_id, exc)
        return None, BackupFailure(book_id=book_id, reason=str(exc))


@router.get("/books", response_model=List[BookResponse])
async def admin_get_books(db: Session = Depends(get_db)):
    """ç®¡ç†å‘˜ï¼šè·å–æ‰€æœ‰ä¹¦ç±åˆ—è¡¨"""
    books = db.query(Book).order_by(Book.created_at.desc()).all()
    logger.info("ğŸ” ç®¡ç†å‘˜è·å–ä¹¦ç±åˆ—è¡¨ total=%s", len(books))
    return books


@router.post("/backup", response_model=BackupResponse)
async def admin_backup_books(payload: BackupRequest, db: Session = Depends(get_db)):
    """ç®¡ç†å‘˜ï¼šæ‰¹é‡å¤‡ä»½ä¹¦ç±"""
    if not payload.book_ids:
        raise HTTPException(status_code=400, detail="è¯·è‡³å°‘æä¾›ä¸€æœ¬ä¹¦ç±ID")

    backups: List[BackupItem] = []
    failed: List[BackupFailure] = []

    for book_id in payload.book_ids:
        backup_item, failure = _backup_single_book(book_id, db)
        if backup_item:
            backups.append(backup_item)
        elif failure:
            failed.append(failure)

    success = len(failed) == 0
    logger.info("ğŸ“¦ ä¹¦ç±å¤‡ä»½å®Œæˆ success=%s backups=%s failed=%s", success, len(backups), len(failed))
    return BackupResponse(success=success, backups=backups, failed=failed)


@router.delete("/books", response_model=AdminDeleteResponse)
async def admin_delete_books(payload: AdminDeleteRequest, db: Session = Depends(get_db)):
    """ç®¡ç†å‘˜ï¼šæ‰¹é‡åˆ é™¤ä¹¦ç±ï¼Œå¿…è¦æ—¶å…ˆè‡ªåŠ¨å¤‡ä»½"""
    if not payload.book_ids:
        raise HTTPException(status_code=400, detail="è¯·è‡³å°‘æä¾›ä¸€æœ¬ä¹¦ç±ID")

    deleted: List[str] = []
    failed: List[AdminDeleteFailure] = []
    backups: List[BackupItem] = []

    for book_id in payload.book_ids:
        logger.info("ğŸ—‘ï¸ æ­£åœ¨åˆ é™¤ä¹¦ç± book_id=%s", book_id)

        if payload.backup_before_delete:
            backup_item, failure = _backup_single_book(book_id, db)
            if failure:
                failed.append(AdminDeleteFailure(book_id=book_id, reason=f"å¤‡ä»½å¤±è´¥ï¼š{failure.reason}"))
                logger.warning("âš ï¸ å¤‡ä»½å¤±è´¥ï¼Œè·³è¿‡åˆ é™¤ book_id=%s", book_id)
                continue
            if backup_item:
                backups.append(backup_item)

        try:
            await delete_book(book_id, db)
            deleted.append(book_id)
            logger.info("âœ… ä¹¦ç±åˆ é™¤æˆåŠŸ book_id=%s", book_id)
        except HTTPException as http_exc:
            failed.append(AdminDeleteFailure(book_id=book_id, reason=str(http_exc.detail)))
            logger.warning("âŒ åˆ é™¤å¤±è´¥ book_id=%s status=%s reason=%s", book_id, http_exc.status_code, http_exc.detail)
        except Exception as exc:
            failed.append(AdminDeleteFailure(book_id=book_id, reason=str(exc)))
            logger.exception("âŒ åˆ é™¤å¤±è´¥ book_id=%s error=%s", book_id, exc)

    success = len(failed) == 0
    response = AdminDeleteResponse(
        success=success,
        deleted=deleted,
        failed=failed,
        backups=backups if payload.backup_before_delete else None
    )
    logger.info(
        "ğŸ§¾ æ‰¹é‡åˆ é™¤å®Œæˆ success=%s deleted=%s failed=%s backups=%s",
        success,
        len(deleted),
        len(failed),
        len(backups) if payload.backup_before_delete else 0
    )
    return response
